## Task 46: `KnowledgeAgentTool` - Full Supabase Client Implementation in `DatabaseManager`

**Focus:**
This task replaces the mock `DatabaseManager` in `python/agents/knowledge_agent/database.py` with a fully functional version that uses the `supabase-py` client to interact with a Supabase instance. This includes:
1.  Storing document chunks and their embeddings in the `rag_pages` table.
2.  Performing vector similarity searches using the `match_rag_pages` RPC function.
3.  Fetching all unique document sources.
4.  Proper error handling for database operations.

**Prerequisites:**
*   A Supabase project must be set up with the `pgvector` extension enabled.
*   The SQL schema from `foundational-rag-agent/rag-example.sql` (which creates the `rag_pages` table and `match_rag_pages` function) must have been applied to the Supabase database.
*   `SUPABASE_URL` and `SUPABASE_KEY` (service role key for write access, anon key might be sufficient for read if RLS is set up for public read) must be correctly configured in the `.env` file.
*   The `supabase` Python library must be in `requirements.txt` and installed.

**File Paths and Code Changes:**

1.  **Verify/Update `requirements.txt`:**
    *   Ensure `supabase>=2.0.0` is present.

2.  **Modify `python/agents/knowledge_agent/database.py`:**
    *   Replace the mock `DatabaseManager` implementation with one that uses `supabase-py`.

    ```python
# python/agents/knowledge_agent/database.py
    import os
    from typing import List, Dict, Any, Optional
    import numpy as np # May not be needed if embeddings are lists
    from supabase import create_client, Client as SupabaseClientLib, PostgrestAPIError
    from dotenv import load_dotenv
    from pathlib import Path
    import asyncio
    import logging

    logger = logging.getLogger(__name__)

    # Load environment variables
    project_root = Path(__file__).resolve().parents[2]
    dotenv_path = project_root / '.env'
    load_dotenv(dotenv_path, override=True)

    class DatabaseManager:
        """
        Manages interaction with the Supabase vector database using pgvector.
        """
        def __init__(self, supabase_url: Optional[str] = None, supabase_key: Optional[str] = None):
            self.url = supabase_url or os.getenv("SUPABASE_URL")
            self.key = supabase_key or os.getenv("SUPABASE_KEY") # This should be the service_role key for inserts/admin

            if not self.url or not self.key:
                logger.error("Supabase URL and Key must be provided via environment variables or arguments.")
                raise ValueError("Supabase URL and Key must be provided.")
            
            try:
                self.client: SupabaseClientLib = create_client(self.url, self.key)
                logger.info(f"DatabaseManager: Successfully connected to Supabase instance at {self.url[:30]}...")
                # Test connection (optional, e.g., by trying to fetch a small, known piece of data or schema)
            except Exception as e:
                logger.error(f"DatabaseManager: Failed to initialize Supabase client: {e}", exc_info=True)
                raise

        async def store_chunks(self, chunks_data: List[Dict[str, Any]], batch_size: int = 50) -> List[Any]: # Return list of Supabase IDs
            """
            Stores chunks with their embeddings and metadata in Supabase.
            chunks_data: list of dicts, each like 
                         {"text": str, "embedding": List[float], "metadata": Dict, "id": str (client-side id, not db id)}
            The actual DB `id` will be auto-generated by Supabase.
            The `url` and `chunk_number` for the DB table should be in `metadata`.
            """
            if not chunks_data:
                return []

            records_to_insert = []
            for chunk_info in chunks_data:
                metadata = chunk_info.get("metadata", {})
                source_url = metadata.get("source_url", metadata.get("url", f"unknown_source_{str(uuid.uuid4())}"))
                chunk_idx = metadata.get("chunk_index", 0) # Ensure chunk_index is present

                if not chunk_info.get("text") or not chunk_info.get("embedding"):
                    logger.warning(f"DatabaseManager: Skipping chunk for {source_url} due to missing text or embedding.")
                    continue

                records_to_insert.append({
                    "url": source_url, 
                    "chunk_number": chunk_idx,
                    "content": chunk_info["text"],
                    "embedding": chunk_info["embedding"], # pgvector handles list of floats
                    "metadata": metadata # Store all other metadata
                })
            
            if not records_to_insert:
                logger.info("DatabaseManager: No valid records to insert after pre-processing.")
                return []

            all_inserted_ids = []
            logger.info(f"DatabaseManager: Attempting to store {len(records_to_insert)} chunks in Supabase in batches of {batch_size}.")
            
            for i in range(0, len(records_to_insert), batch_size):
                batch = records_to_insert[i:i+batch_size]
                try:
                    # The Supabase Python client's insert is synchronous. Use to_thread.
                    response = await asyncio.to_thread(
                        self.client.table("rag_pages").insert(batch).execute
                    )
                    if response.data:
                        inserted_ids_batch = [record['id'] for record in response.data]
                        all_inserted_ids.extend(inserted_ids_batch)
                        logger.info(f"DatabaseManager: Stored batch {i//batch_size + 1}, {len(inserted_ids_batch)} chunks in Supabase.")
                    else:
                        error_info = getattr(response, 'error', "Unknown error during insert")
                        logger.error(f"DatabaseManager: Failed to store batch {i//batch_size + 1}. Supabase error: {error_info}. Data (first item): {batch[0] if batch else 'N/A'}")
                except PostgrestAPIError as pae: # Specific Supabase error
                    logger.error(f"DatabaseManager: PostgrestAPIError storing batch {i//batch_size + 1}: {pae.message} (Code: {pae.code}, Details: {pae.details}, Hint: {pae.hint})", exc_info=True)
                except Exception as e:
                    logger.error(f"DatabaseManager: General exception storing batch {i//batch_size + 1}: {e}", exc_info=True)
            
            logger.info(f"DatabaseManager: Finished storing chunks. Total successfully inserted IDs: {len(all_inserted_ids)}.")
            return all_inserted_ids


        async def semantic_search(
            self, 
            query_embedding: List[float], 
            limit: int, 
            filter_metadata: Optional[Dict] = None # This is the `filter` arg for the RPC function
        ) -> List[Dict[str, Any]]:
            """Performs semantic search using the match_rag_pages RPC function."""
            if not query_embedding or sum(abs(x) for x in query_embedding) == 0:
                logger.warning("DatabaseManager: Query embedding is zero or empty. Returning no results.")
                return []

            rpc_params = {
                "query_embedding": query_embedding, # Must match SQL function param name
                "match_count": limit,             # Must match SQL function param name
                "filter": filter_metadata or {}   # Must match SQL function param name, ensure it's a dict
            }
            
            logger.debug(f"DatabaseManager: Calling RPC 'match_rag_pages' with params: query_embedding_dim={len(query_embedding)}, match_count={limit}, filter={rpc_params['filter']}")

            try:
                response = await asyncio.to_thread(
                    self.client.rpc("match_rag_pages", rpc_params).execute
                )
                if response.data:
                    logger.info(f"DatabaseManager: RPC 'match_rag_pages' returned {len(response.data)} results.")
                    # The RPC function should return: id, url, chunk_number, content, metadata, similarity
                    return response.data
                else:
                    error_info = getattr(response, 'error', "Unknown error from RPC")
                    logger.warning(f"DatabaseManager: No results or error from RPC 'match_rag_pages'. Error: {error_info}")
                    return []
            except PostgrestAPIError as pae:
                 logger.error(f"DatabaseManager: PostgrestAPIError during RPC 'match_rag_pages': {pae.message}", exc_info=True)
            except Exception as e:
                logger.error(f"DatabaseManager: Exception during RPC 'match_rag_pages': {e}", exc_info=True)
            return []

        async def get_all_sources(self) -> List[str]:
            """Fetches all unique 'url' values from the rag_pages table (which represent document sources)."""
            try:
                # This might be slow for very large tables. Consider if a dedicated sources table is needed.
                # For now, a distinct select on url.
                # The Python client might not directly support DISTINCT in select string easily for ORM-like methods.
                # Using RPC or a view might be better for large scale.
                # A simpler approach for smaller datasets:
                response = await asyncio.to_thread(
                    self.client.table("rag_pages").select("url", count="exact").execute
                )
                # The above gets all URLs, then we make them unique in Python.
                # If performance is an issue, create a view or function in Supabase for distinct URLs.
                
                if response.data:
                    sources = sorted(list(set(item["url"] for item in response.data if item and "url" in item)))
                    logger.info(f"DatabaseManager: Retrieved {len(sources)} unique sources.")
                    return sources
                else:
                    error_info = getattr(response, 'error', "Unknown error")
                    logger.warning(f"DatabaseManager: Failed to get sources. Error: {error_info}")
                    return []
            except Exception as e:
                logger.error(f"DatabaseManager: Exception during get_all_sources: {e}", exc_info=True)
                return []
```

3.  **Verify/Update `python/agents/knowledge_agent/agent.py` (`KnowledgeRAGAgent`):**
    *   Ensure it correctly instantiates `DatabaseManager`.
    *   Ensure the data passed to `db_manager.store_chunks` in `ingest_document_chunks` is in the correct format (especially the `metadata` field which should contain `source_url` and `chunk_index`).
    *   The `query_knowledge_base` method should correctly interpret the results from the real `db_manager.semantic_search`.

    ```python
# python/agents/knowledge_agent/agent.py
    # ... (imports)
    class KnowledgeRAGAgent:
        def __init__(self, 
                     database_manager: Optional[DatabaseManager] = None, 
                     # ... (other params as in Task 17)
                    ):
            self.db_manager = database_manager or DatabaseManager() # Now uses real Supabase client
            # ... (rest of __init__)

        async def ingest_document_chunks(self, chunks_data: List[Dict[str, Any]]) -> Dict[str, Any]:
            """
            Ingests pre-processed chunks. Assumes chunks_data items have:
            "id" (client-side unique ID for this batch of chunks), 
            "text", 
            "embedding" (List[float]), 
            "metadata" (which MUST include "source_url" and "chunk_index" for the DB table)
            """
            logger.info(f"KnowledgeRAGAgent: Ingesting {len(chunks_data)} chunks via real DatabaseManager.")
            
            # Validate chunks_data structure for required metadata by DatabaseManager
            valid_chunks_for_db = []
            for i, chunk_d in enumerate(chunks_data):
                if not chunk_d.get("metadata") or \
                   "source_url" not in chunk_d["metadata"] or \
                   "chunk_index" not in chunk_d["metadata"]:
                    logger.warning(f"Skipping chunk {i} (ID: {chunk_d.get('id')}) due to missing source_url or chunk_index in metadata.")
                    continue
                if not chunk_d.get("text") or not chunk_d.get("embedding"):
                    logger.warning(f"Skipping chunk {i} (ID: {chunk_d.get('id')}) due to missing text or embedding.")
                    continue
                valid_chunks_for_db.append(chunk_d)

            if not valid_chunks_for_db:
                return {"status": "error", "message": "No valid chunks provided for ingestion.", "count": 0}

            stored_db_ids = await self.db_manager.store_chunks(valid_chunks_for_db)
            return {"status": "success", "ingested_supabase_ids": stored_db_ids, "count": len(stored_db_ids)}

        # query_knowledge_base from Task 17 should largely work, as it expects a list of dicts
        # from retriever, which in turn gets it from db_manager.semantic_search.
        # Just ensure field names are consistent ("similarity" vs "similarity_score").
        # The RPC function `match_rag_pages` returns `similarity`.
        # The previous mock DatabaseManager returned `similarity_score`. Let's standardize.
        # The RPC function returns `metadata` as a JSONB column.
```

4.  **Verify `python/tools/knowledge_agent_tool.py`:**
    *   Ensure it correctly instantiates `KnowledgeRAGAgent` (which now uses the real `DatabaseManager`).
    *   When calling `ingest_chunks`, ensure the `chunks_data` being passed has the correct structure, especially that each item's `metadata` sub-dictionary includes `source_url` and `chunk_index`. This data comes from `WebCrawlerTool` -> `HierarchicalChunker`.

    ```python
# In python/agents/web_crawler/chunker.py, HierarchicalChunker.chunk_document already produces:
    # chunk_metadata = {
    #     **base_metadata, 
    #     "source_url": source_url, # This is good
    #     "document_title": doc_title,
    #     "chunk_index": chunk_index_counter, # This is good
    #     "headers": ...,
    #     "char_count": ...
    # }
    # final_chunks_data.append({
    #     "id": f"{source_url}_chunk_{chunk_index_counter}", # Client-side ID for this batch
    #     "text": sub_chunk_text,
    #     "metadata": chunk_metadata 
    #     # Embedding is added by KnowledgeAgentTool._ingest_chunks if not present
    # })
    # This structure is compatible.
```

**Dependencies/Prerequisites:**
*   Tasks 1-45 completed.
*   `supabase-py` library installed.
*   Supabase project set up with pgvector and the `rag_pages` table and `match_rag_pages` function.
*   `SUPABASE_URL` and `SUPABASE_KEY` (service role) configured in `.env`.
*   `EmbeddingGenerator` (Task 14/42) is functional for producing embeddings.

**Integration with Agent Zero:**
*   The `KnowledgeAgentTool` now performs real database operations for its RAG capabilities.
*   Chunks of text processed by `WebCrawlerTool` can now be permanently stored and made searchable.
*   The agent's ability to answer questions based on ingested documentation becomes fully functional (excluding advanced LLM prompting for generation, which is Task 17's focus but can be refined).

**Chatterbox TTS Integration Requirements for this Task:**
*   None directly.

**Docker Compatibility:**
*   Add `supabase` to `requirements.txt`.
*   The Docker container needs network access to the Supabase instance.
*   `SUPABASE_URL` and `SUPABASE_KEY` environment variables must be available to the container.

**Summary of Task 46:**
This task is a major step, making the `KnowledgeAgentTool` fully interact with a real Supabase database for its RAG backend. It replaces all mock database logic with actual calls to Supabase for storing document chunks with their embeddings and performing vector similarity searches. This makes the knowledge base persistent and querying functional. The next step for `KnowledgeAgentTool` would be to refine the LLM interaction for generating answers from the retrieved context.

Please confirm to proceed.Okay, let's proceed with Task 46.

## Task 46: `KnowledgeAgentTool` - Full Supabase Client Implementation in `DatabaseManager`

**Focus:**
This task replaces the mock `DatabaseManager` in `python/agents/knowledge_agent/database.py` with a fully functional version that uses the `supabase-py` client to interact with a Supabase instance. This includes:
1.  Storing document chunks and their embeddings in the `rag_pages` table.
2.  Performing vector similarity searches using the `match_rag_pages` RPC function.
3.  Fetching all unique document sources.
4.  Proper error handling for database operations.

**Prerequisites:**
*   A Supabase project must be set up with the `pgvector` extension enabled.
*   The SQL schema from `foundational-rag-agent/rag-example.sql` (which creates the `rag_pages` table and `match_rag_pages` function) must have been applied to the Supabase database.
*   `SUPABASE_URL` and `SUPABASE_KEY` (service role key for write access, anon key might be sufficient for read if RLS is set up for public read) must be correctly configured in the `.env` file.
*   The `supabase` Python library must be in `requirements.txt` and installed.

**File Paths and Code Changes:**

1.  **Verify/Update `requirements.txt`:**
    *   Ensure `supabase>=2.0.0` is present.

2.  **Modify `python/agents/knowledge_agent/database.py`:**
    *   Replace the mock `DatabaseManager` implementation with one that uses `supabase-py`.

    ```python
    # python/agents/knowledge_agent/database.py
    import os
    from typing import List, Dict, Any, Optional
    import numpy as np # May not be needed if embeddings are lists
    from supabase import create_client, Client as SupabaseClientLib, PostgrestAPIError
    from dotenv import load_dotenv
    from pathlib import Path
    import asyncio
    import logging

    logger = logging.getLogger(__name__)

    # Load environment variables
    project_root = Path(__file__).resolve().parents[2]
    dotenv_path = project_root / '.env'
    load_dotenv(dotenv_path, override=True)

    class DatabaseManager:
        """
        Manages interaction with the Supabase vector database using pgvector.
        """
        def __init__(self, supabase_url: Optional[str] = None, supabase_key: Optional[str] = None):
            self.url = supabase_url or os.getenv("SUPABASE_URL")
            self.key = supabase_key or os.getenv("SUPABASE_KEY") # This should be the service_role key for inserts/admin

            if not self.url or not self.key:
                logger.error("Supabase URL and Key must be provided via environment variables or arguments.")
                raise ValueError("Supabase URL and Key must be provided.")
            
            try:
                self.client: SupabaseClientLib = create_client(self.url, self.key)
                logger.info(f"DatabaseManager: Successfully connected to Supabase instance at {self.url[:30]}...")
                # Test connection (optional, e.g., by trying to fetch a small, known piece of data or schema)
            except Exception as e:
                logger.error(f"DatabaseManager: Failed to initialize Supabase client: {e}", exc_info=True)
                raise

        async def store_chunks(self, chunks_data: List[Dict[str, Any]], batch_size: int = 50) -> List[Any]: # Return list of Supabase IDs
            """
            Stores chunks with their embeddings and metadata in Supabase.
            chunks_data: list of dicts, each like 
                         {"text": str, "embedding": List[float], "metadata": Dict, "id": str (client-side id, not db id)}
            The actual DB `id` will be auto-generated by Supabase.
            The `url` and `chunk_number` for the DB table should be in `metadata`.
            """
            if not chunks_data:
                return []

            records_to_insert = []
            for chunk_info in chunks_data:
                metadata = chunk_info.get("metadata", {})
                source_url = metadata.get("source_url", metadata.get("url", f"unknown_source_{str(uuid.uuid4())}"))
                chunk_idx = metadata.get("chunk_index", 0) # Ensure chunk_index is present

                if not chunk_info.get("text") or not chunk_info.get("embedding"):
                    logger.warning(f"DatabaseManager: Skipping chunk for {source_url} due to missing text or embedding.")
                    continue

                records_to_insert.append({
                    "url": source_url, 
                    "chunk_number": chunk_idx,
                    "content": chunk_info["text"],
                    "embedding": chunk_info["embedding"], # pgvector handles list of floats
                    "metadata": metadata # Store all other metadata
                })
            
            if not records_to_insert:
                logger.info("DatabaseManager: No valid records to insert after pre-processing.")
                return []

            all_inserted_ids = []
            logger.info(f"DatabaseManager: Attempting to store {len(records_to_insert)} chunks in Supabase in batches of {batch_size}.")
            
            for i in range(0, len(records_to_insert), batch_size):
                batch = records_to_insert[i:i+batch_size]
                try:
                    # The Supabase Python client's insert is synchronous. Use to_thread.
                    response = await asyncio.to_thread(
                        self.client.table("rag_pages").insert(batch).execute
                    )
                    if response.data:
                        inserted_ids_batch = [record['id'] for record in response.data]
                        all_inserted_ids.extend(inserted_ids_batch)
                        logger.info(f"DatabaseManager: Stored batch {i//batch_size + 1}, {len(inserted_ids_batch)} chunks in Supabase.")
                    else:
                        error_info = getattr(response, 'error', "Unknown error during insert")
                        logger.error(f"DatabaseManager: Failed to store batch {i//batch_size + 1}. Supabase error: {error_info}. Data (first item): {batch[0] if batch else 'N/A'}")
                except PostgrestAPIError as pae: # Specific Supabase error
                    logger.error(f"DatabaseManager: PostgrestAPIError storing batch {i//batch_size + 1}: {pae.message} (Code: {pae.code}, Details: {pae.details}, Hint: {pae.hint})", exc_info=True)
                except Exception as e:
                    logger.error(f"DatabaseManager: General exception storing batch {i//batch_size + 1}: {e}", exc_info=True)
            
            logger.info(f"DatabaseManager: Finished storing chunks. Total successfully inserted IDs: {len(all_inserted_ids)}.")
            return all_inserted_ids


        async def semantic_search(
            self, 
            query_embedding: List[float], 
            limit: int, 
            filter_metadata: Optional[Dict] = None # This is the `filter` arg for the RPC function
        ) -> List[Dict[str, Any]]:
            """Performs semantic search using the match_rag_pages RPC function."""
            if not query_embedding or sum(abs(x) for x in query_embedding) == 0:
                logger.warning("DatabaseManager: Query embedding is zero or empty. Returning no results.")
                return []

            rpc_params = {
                "query_embedding": query_embedding, # Must match SQL function param name
                "match_count": limit,             # Must match SQL function param name
                "filter": filter_metadata or {}   # Must match SQL function param name, ensure it's a dict
            }
            
            logger.debug(f"DatabaseManager: Calling RPC 'match_rag_pages' with params: query_embedding_dim={len(query_embedding)}, match_count={limit}, filter={rpc_params['filter']}")

            try:
                response = await asyncio.to_thread(
                    self.client.rpc("match_rag_pages", rpc_params).execute
                )
                if response.data:
                    logger.info(f"DatabaseManager: RPC 'match_rag_pages' returned {len(response.data)} results.")
                    # The RPC function should return: id, url, chunk_number, content, metadata, similarity
                    return response.data
                else:
                    error_info = getattr(response, 'error', "Unknown error from RPC")
                    logger.warning(f"DatabaseManager: No results or error from RPC 'match_rag_pages'. Error: {error_info}")
                    return []
            except PostgrestAPIError as pae:
                 logger.error(f"DatabaseManager: PostgrestAPIError during RPC 'match_rag_pages': {pae.message}", exc_info=True)
            except Exception as e:
                logger.error(f"DatabaseManager: Exception during RPC 'match_rag_pages': {e}", exc_info=True)
            return []

        async def get_all_sources(self) -> List[str]:
            """Fetches all unique 'url' values from the rag_pages table (which represent document sources)."""
            try:
                # This might be slow for very large tables. Consider if a dedicated sources table is needed.
                # For now, a distinct select on url.
                # The Python client might not directly support DISTINCT in select string easily for ORM-like methods.
                # Using RPC or a view might be better for large scale.
                # A simpler approach for smaller datasets:
                response = await asyncio.to_thread(
                    self.client.table("rag_pages").select("url", count="exact").execute
                )
                # The above gets all URLs, then we make them unique in Python.
                # If performance is an issue, create a view or function in Supabase for distinct URLs.
                
                if response.data:
                    sources = sorted(list(set(item["url"] for item in response.data if item and "url" in item)))
                    logger.info(f"DatabaseManager: Retrieved {len(sources)} unique sources.")
                    return sources
                else:
                    error_info = getattr(response, 'error', "Unknown error")
                    logger.warning(f"DatabaseManager: Failed to get sources. Error: {error_info}")
                    return []
            except Exception as e:
                logger.error(f"DatabaseManager: Exception during get_all_sources: {e}", exc_info=True)
                return []

    ```

3.  **Verify/Update `python/agents/knowledge_agent/agent.py` (`KnowledgeRAGAgent`):**
    *   Ensure it correctly instantiates `DatabaseManager`.
    *   Ensure the data passed to `db_manager.store_chunks` in `ingest_document_chunks` is in the correct format (especially the `metadata` field which should contain `source_url` and `chunk_index`).
    *   The `query_knowledge_base` method should correctly interpret the results from the real `db_manager.semantic_search`.

    ```python
    # python/agents/knowledge_agent/agent.py
    # ... (imports)
    class KnowledgeRAGAgent:
        def __init__(self, 
                     database_manager: Optional[DatabaseManager] = None, 
                     # ... (other params as in Task 17)
                    ):
            self.db_manager = database_manager or DatabaseManager() # Now uses real Supabase client
            # ... (rest of __init__)

        async def ingest_document_chunks(self, chunks_data: List[Dict[str, Any]]) -> Dict[str, Any]:
            """
            Ingests pre-processed chunks. Assumes chunks_data items have:
            "id" (client-side unique ID for this batch of chunks), 
            "text", 
            "embedding" (List[float]), 
            "metadata" (which MUST include "source_url" and "chunk_index" for the DB table)
            """
            logger.info(f"KnowledgeRAGAgent: Ingesting {len(chunks_data)} chunks via real DatabaseManager.")
            
            # Validate chunks_data structure for required metadata by DatabaseManager
            valid_chunks_for_db = []
            for i, chunk_d in enumerate(chunks_data):
                if not chunk_d.get("metadata") or \
                   "source_url" not in chunk_d["metadata"] or \
                   "chunk_index" not in chunk_d["metadata"]:
                    logger.warning(f"Skipping chunk {i} (ID: {chunk_d.get('id')}) due to missing source_url or chunk_index in metadata.")
                    continue
                if not chunk_d.get("text") or not chunk_d.get("embedding"):
                    logger.warning(f"Skipping chunk {i} (ID: {chunk_d.get('id')}) due to missing text or embedding.")
                    continue
                valid_chunks_for_db.append(chunk_d)

            if not valid_chunks_for_db:
                return {"status": "error", "message": "No valid chunks provided for ingestion.", "count": 0}

            stored_db_ids = await self.db_manager.store_chunks(valid_chunks_for_db)
            return {"status": "success", "ingested_supabase_ids": stored_db_ids, "count": len(stored_db_ids)}

        # query_knowledge_base from Task 17 should largely work, as it expects a list of dicts
        # from retriever, which in turn gets it from db_manager.semantic_search.
        # Just ensure field names are consistent ("similarity" vs "similarity_score").
        # The RPC function `match_rag_pages` returns `similarity`.
        # The previous mock DatabaseManager returned `similarity_score`. Let's standardize.
        # The RPC function returns `metadata` as a JSONB column.
    ```

4.  **Verify `python/tools/knowledge_agent_tool.py`:**
    *   Ensure it correctly instantiates `KnowledgeRAGAgent` (which now uses the real `DatabaseManager`).
    *   When calling `ingest_chunks`, ensure the `chunks_data` being passed has the correct structure, especially that each item's `metadata` sub-dictionary includes `source_url` and `chunk_index`. This data comes from `WebCrawlerTool` -> `HierarchicalChunker`.

    ```python
    # In python/agents/web_crawler/chunker.py, HierarchicalChunker.chunk_document already produces:
    # chunk_metadata = {
    #     **base_metadata, 
    #     "source_url": source_url, # This is good
    #     "document_title": doc_title,
    #     "chunk_index": chunk_index_counter, # This is good
    #     "headers": ...,
    #     "char_count": ...
    # }
    # final_chunks_data.append({
    #     "id": f"{source_url}_chunk_{chunk_index_counter}", # Client-side ID for this batch
    #     "text": sub_chunk_text,
    #     "metadata": chunk_metadata 
    #     # Embedding is added by KnowledgeAgentTool._ingest_chunks if not present
    # })
    # This structure is compatible.
    ```

**Dependencies/Prerequisites:**
*   Tasks 1-45 completed.
*   `supabase-py` library installed.
*   Supabase project set up with pgvector and the `rag_pages` table and `match_rag_pages` function.
*   `SUPABASE_URL` and `SUPABASE_KEY` (service role) configured in `.env`.
*   `EmbeddingGenerator` (Task 14/42) is functional for producing embeddings.

**Integration with Agent Zero:**
*   The `KnowledgeAgentTool` now performs real database operations for its RAG capabilities.
*   Chunks of text processed by `WebCrawlerTool` can now be permanently stored and made searchable.
*   The agent's ability to answer questions based on ingested documentation becomes fully functional (excluding advanced LLM prompting for generation, which is Task 17's focus but can be refined).

**Chatterbox TTS Integration Requirements for this Task:**
*   None directly.

**Docker Compatibility:**
*   Add `supabase` to `requirements.txt`.
*   The Docker container needs network access to the Supabase instance.
*   `SUPABASE_URL` and `SUPABASE_KEY` environment variables must be available to the container.

**Summary of Task 46:**
This task is a major step, making the `KnowledgeAgentTool` fully interact with a real Supabase database for its RAG backend. It replaces all mock database logic with actual calls to Supabase for storing document chunks with their embeddings and performing vector similarity searches. This makes the knowledge base persistent and querying functional. The next step for `KnowledgeAgentTool` would be to refine the LLM interaction for generating answers from the retrieved context.

Please confirm to proceed.