## Task 19: Implement Real Logic for `ChatterboxTTSTool` - Model Loading and Basic TTS Generation

**Focus:**
This task transitions the `ChatterboxTTSTool` from placeholder logic to a functional implementation for the `generate_speech` action. This involves:
1.  Integrating the actual `chatterbox-tts` library.
2.  Implementing model loading for `ChatterboxTTS` within `ChatterboxTTSHandler`.
3.  Implementing the `generate_speech` method in `ChatterboxTTSHandler` to perform actual audio synthesis.
4.  Handling audio data (e.g., saving to a temporary file and returning a path, or returning base64 encoded data).
5.  Ensuring Perth watermarking is applied.

**Assumptions:**
*   The `chatterbox-tts` library and its models are accessible (e.g., via Hugging Face Hub or local paths).
*   Dependencies of `chatterbox-tts` (like `torch`, `torchaudio`, `librosa`, `s3tokenizer`, `resemble-perth`) are managed.

**File Paths and Code Changes:**

1.  **Modify `requirements.txt`:**
    *   Add `chatterbox-tts` and its core dependencies if not already covered by other tools (e.g., `torch`).
    *   The `pyproject.toml` from `chatterbox full code.md` lists:
        ```
numpy>=1.26.0
        librosa==0.11.0
        s3tokenizer
        torch==2.6.0
        torchaudio==2.6.0
        transformers==4.46.3
        diffusers==0.29.0
        resemble-perth==1.0.1
        conformer==0.3.2
        safetensors==0.5.3
```
    *   We need to ensure these (or compatible versions) are in `requirements.txt`.

    ```
# requirements.txt
    # ... (existing requirements)
    chatterbox-tts # This should pull its own dependencies as defined in its pyproject.toml
    # If chatterbox-tts is not on PyPI, or for local dev:
    # -e ./path/to/chatterbox_local_clone # If installing from a local clone
    # Or list its specific dependencies if installing them manually:
    # numpy>=1.26.0
    # librosa==0.11.0
    # s3tokenizer
    # torch # Ensure version compatibility with other tools
    # torchaudio # Ensure version compatibility
    # transformers
    # diffusers
    # resemble-perth==1.0.1
    # conformer==0.3.2
    # safetensors
```
    For this task, let's assume `pip install chatterbox-tts` (if it were a package) or `pip install -e /path/to/chatterbox_src_dir` would handle its dependencies. We will use the direct import from `chatterbox.tts` as if it's installed.

2.  **Modify `python/agents/tts_agent/chatterbox_handler.py`:**
    *   Replace the mock `ChatterboxTTSHandler` with a real one that loads and uses `chatterbox.tts.ChatterboxTTS`.

    ```python
# python/agents/tts_agent/chatterbox_handler.py
    import asyncio
    from typing import Dict, Any, Optional, Tuple
    import numpy as np
    import torch
    import librosa # For loading audio prompt if needed by Chatterbox internal
    import tempfile
    import os
    from pathlib import Path

    try:
        from chatterbox.tts import ChatterboxTTS as RealChatterboxTTS, punc_norm
        from chatterbox.models.s3gen import S3GEN_SR # Sample rate from chatterbox
        CHATTERBOX_AVAILABLE = True
    except ImportError:
        print("ChatterboxTTSHandler: chatterbox-tts library not found. TTS tool will not be functional.")
        CHATTERBOX_AVAILABLE = False
        # Placeholder for S3GEN_SR if library not found
        S3GEN_SR = 24000 
        class RealChatterboxTTS: # Placeholder
            def __init__(self, *args, **kwargs): pass
            @classmethod
            def from_pretrained(cls, *args, **kwargs): return cls()
            def generate(self, *args, **kwargs): 
                num_samples = int(S3GEN_SR * 2.0)
                return torch.zeros(1, num_samples, dtype=torch.float32) # Batch dim, samples
            @property
            def sr(self): return S3GEN_SR

    class ChatterboxTTSHandler:
        """
        Handles Text-to-Speech generation using the real Chatterbox library.
        """
        _model_instance: Optional[RealChatterboxTTS] = None
        _model_lock = asyncio.Lock()

        def __init__(self, device: str = "cpu"):
            self.device = device
            self.sr = S3GEN_SR # Sample rate of Chatterbox output
            # Model loading is deferred to ensure it happens in the correct async context / process
            print(f"ChatterboxTTSHandler: Initialized for device: {device}. Model will be loaded on first use.")

        async def _ensure_model_loaded(self):
            async with self._model_lock:
                if not CHATTERBOX_AVAILABLE:
                    raise RuntimeError("Chatterbox library is not installed. TTS functionality is unavailable.")
                if ChatterboxTTSHandler._model_instance is None:
                    print(f"ChatterboxTTSHandler: Loading ChatterboxTTS model for device '{self.device}'...")
                    try:
                        # from_pretrained might be synchronous, wrap if necessary
                        ChatterboxTTSHandler._model_instance = await asyncio.to_thread(
                            RealChatterboxTTS.from_pretrained, device=self.device
                        )
                        print("ChatterboxTTSHandler: Model loaded successfully.")
                    except Exception as e:
                        print(f"ChatterboxTTSHandler: Failed to load ChatterboxTTS model: {e}")
                        import traceback
                        traceback.print_exc()
                        ChatterboxTTSHandler._model_instance = None # Ensure it's None on failure
                        raise # Re-raise exception to signal failure
                
                if ChatterboxTTSHandler._model_instance is None:
                     raise RuntimeError("ChatterboxTTS model could not be loaded.")
            return ChatterboxTTSHandler._model_instance

        async def generate_speech(self, text: str, audio_prompt_path: Optional[str] = None, 
                                  exaggeration: float = 0.5, cfg_weight: float = 0.5, 
                                  temperature: float = 0.8) -> Tuple[int, bytes]:
            """Generates speech using ChatterboxTTS model."""
            if not CHATTERBOX_AVAILABLE:
                 raise RuntimeError("Chatterbox library is not installed. Cannot generate speech.")

            model = await self._ensure_model_loaded()
            
            print(f"ChatterboxTTSHandler: Generating speech for text: '{text[:50]}...' with prompt: {audio_prompt_path}")
            
            # Normalize punctuation as per Chatterbox example
            normalized_text = punc_norm(text)

            # Chatterbox's model.generate is synchronous
            # We need to run it in a thread pool executor to avoid blocking the asyncio loop
            try:
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate,
                    text=normalized_text,
                    audio_prompt_path=audio_prompt_path, # Path to a .wav file
                    exaggeration=exaggeration,
                    cfg_weight=cfg_weight,
                    temperature=temperature
                ) # Returns torch.Tensor of shape (1, num_samples)
            except Exception as e:
                print(f"ChatterboxTTSHandler: Error during model.generate: {e}")
                import traceback
                traceback.print_exc()
                raise

            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0:
                print("ChatterboxTTSHandler: Model generated empty audio.")
                raise ValueError("TTS model generated empty audio.")

            # Assuming batch size of 1 from model.generate output
            wav_tensor = wav_tensor_batched.squeeze(0).cpu() # (num_samples,)
            audio_numpy = wav_tensor.numpy()

            # Watermarking is handled internally by ChatterboxTTS.generate if it uses its own watermarker.
            # The example in chatterbox/tts.py shows:
            #   wav = model.generate(...)
            #   watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
            # So, the `model.generate` in ChatterboxTTS already returns watermarked audio.
            # If `model.generate` does NOT return watermarked audio, we'd do it here:
            # watermarker = perth.PerthImplicitWatermarker() # From chatterbox.tts
            # watermarked_audio_numpy = watermarker.apply_watermark(audio_numpy, sample_rate=self.sr)
            # audio_bytes = watermarked_audio_numpy.astype(np.float32).tobytes()
            
            audio_bytes = audio_numpy.astype(np.float32).tobytes() # Assuming generate() already watermarked
            
            print(f"ChatterboxTTSHandler: Speech generated ({len(audio_bytes)} bytes, SR: {self.sr}).")
            return self.sr, audio_bytes

    # ChatterboxVCHandler would be updated similarly if VC is a priority.
    # For now, focusing on TTS. If VC is needed, it would follow a similar pattern:
    # class ChatterboxVCHandler: ... load VC model, implement convert_voice ...
    # For this task, we'll leave ChatterboxVCHandler as its mock version from Task 10
    # to keep the scope manageable.
    class ChatterboxVCHandler: # Keeping mock from Task 10 for now
        def __init__(self, device: str = "cpu"):
            self.device = device
            self.sr = S3GEN_SR 
            print(f"ChatterboxVCHandler (Mock - VC Not Implemented in this task) initialized on device: {device}")
        
        async def _mock_audio_data(self, duration_seconds: float = 2.0) -> Tuple[int, np.ndarray]:
            num_samples = int(self.sr * duration_seconds)
            audio_data = np.zeros(num_samples, dtype=np.float32)
            return self.sr, audio_data

        async def convert_voice(self, source_audio_path: str, target_voice_path: str) -> Tuple[int, bytes]:
            print(f"ChatterboxVCHandler (Mock): VC from '{source_audio_path}' to target '{target_voice_path}'. NOT IMPLEMENTED.")
            sr, audio_np = await self._mock_audio_data(duration_seconds=3.0) 
            audio_bytes = audio_np.tobytes()
            return sr, audio_bytes
```

4.  **Modify `python/tools/chatterbox_tts_tool.py`:**
    *   Ensure it correctly instantiates and calls the updated `ChatterboxTTSHandler`.
    *   The response should now contain real (though potentially temporary) audio data. A common way to return audio is to save it to a temporary file in `tmp/tts_output/` and return the file path, or return base64 encoded data. For AG-UI, a URL to the audio or base64 might be suitable.
    *   We need a mechanism to serve these temporary files if paths are returned. Agent Zero already has `/api/download_work_dir_file` and `/api/image_get`. A similar one for TTS audio might be needed or adapt an existing one. For now, let's return base64.

    ```python
# python/tools/chatterbox_tts_tool.py
    # ... (imports as before)
    import tempfile # For saving audio temporarily if needed
    from pathlib import Path # For path manipulation

    class ChatterboxTTSTool(Tool):
        _tts_handler_instance: Optional[ChatterboxTTSHandler] = None
        _vc_handler_instance: Optional[ChatterboxVCHandler] = None
        _handler_lock = asyncio.Lock()

        @classmethod
        async def get_tts_handler(cls, device: str) -> ChatterboxTTSHandler:
            async with cls._handler_lock:
                if cls._tts_handler_instance is None:
                    cls._tts_handler_instance = ChatterboxTTSHandler(device=device)
                # Ensure model is loaded within handler if not already
                await cls._tts_handler_instance._ensure_model_loaded()
            return cls._tts_handler_instance

        @classmethod
        async def get_vc_handler(cls, device: str) -> ChatterboxVCHandler: # VC still mock
            async with cls._handler_lock:
                if cls._vc_handler_instance is None:
                    cls._vc_handler_instance = ChatterboxVCHandler(device=device)
            return cls._vc_handler_instance


        def __init__(self, agent, **kwargs):
            super().__init__(agent, name="chatterbox_tts_tool",
                             description="Generates speech from text (TTS) or performs voice conversion (VC) using Chatterbox models.",
                             args_schema=None,
                             **kwargs)
            self.device = agent.config.get("tts_device", agent.config.get("device", "cpu")) # Get device from agent config
            # Handlers will be fetched on demand to ensure model loading happens in async context
            print(f"ChatterboxTTSTool initialized for agent {agent.agent_name}. Handlers will use device: {self.device}")

        # ... (_emit_tts_event method as before)

        async def execute(self, action: str, **kwargs) -> ToolResponse:
            # ... (action routing as before) ...
            try:
                if action == "generate_speech":
                    tts_handler = await self.get_tts_handler(self.device)
                    text = kwargs.get("text")
                    # ... (get other params: audio_prompt_path, exaggeration, etc.)
                    if not text: return ToolResponse("Error: 'text' is required for generate_speech.", error=True)
                    
                    audio_prompt_path = kwargs.get("audio_prompt_path") 
                    exaggeration = float(kwargs.get("exaggeration", 0.5))
                    cfg_weight = float(kwargs.get("cfg_weight", 0.5))
                    temperature = float(kwargs.get("temperature", 0.8))
                    
                    return await self._generate_speech(tts_handler, text, audio_prompt_path, exaggeration, cfg_weight, temperature)
                
                elif action == "convert_voice":
                    vc_handler = await self.get_vc_handler(self.device) # Still mock
                    # ... (get source_audio_path, target_voice_path)
                    source_audio_path = kwargs.get("source_audio_path")
                    target_voice_path = kwargs.get("target_voice_path")
                    if not source_audio_path or not target_voice_path:
                        return ToolResponse("Error: 'source_audio_path' and 'target_voice_path' required.", error=True)
                    return await self._convert_voice(vc_handler, source_audio_path, target_voice_path) # Calls mock
                # ...
            # ... (exception handling as before)
            except RuntimeError as rne: # Catch model loading errors specifically
                error_message = f"ChatterboxTTSTool runtime error (likely model loading) during action '{action}': {str(rne)}"
                print(error_message)
                await self._emit_tts_event(action, "error", {"error": str(rne), "type": "RuntimeError"})
                return ToolResponse(message=error_message, error=True)
            except Exception as e: # General errors
                # ... (as before)
                import traceback
                error_message = f"ChatterboxTTSTool error during action '{action}': {str(e)}\n{traceback.format_exc()}"
                print(error_message)
                await self._emit_tts_event(action, "error", {"error": str(e)})
                return ToolResponse(message=error_message, error=True)


        async def _generate_speech(self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
                                   exaggeration: float, cfg_weight: float, temperature: float) -> ToolResponse:
            await self._emit_tts_event("generate_speech", "starting", {"text_length": len(text), "prompt": bool(audio_prompt_path)})
            
            sr, audio_bytes = await tts_handler.generate_speech(
                text, audio_prompt_path, exaggeration, cfg_weight, temperature
            )
            
            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
            
            # Create a temporary file path for the audio.
            # The UI will need an endpoint to fetch this if it doesn't handle base64 directly.
            # Agent Zero's work_dir is /root in Docker, or ./work_dir locally.
            # Let's use a subdirectory in tmp for TTS outputs for now.
            
            # Ensure tmp/tts_output directory exists
            tts_output_dir = Path(self.agent.context.get_custom_data("work_dir_path", "work_dir")) / "tmp" / "tts_output"
            tts_output_dir.mkdir(parents=True, exist_ok=True) # Agent Zero might need a helper for this

            temp_audio_filename = f"tts_output_{uuid.uuid4()}.wav"
            temp_audio_filepath = tts_output_dir / temp_audio_filename
            
            try:
                # Save the raw bytes as a .wav file using torchaudio or soundfile
                # For raw bytes (assuming float32 PCM), need to construct a WAV header or use a library
                # Torchaudio is a dependency of chatterbox, so it should be available.
                import torchaudio
                # Convert raw bytes back to tensor
                audio_tensor = torch.from_numpy(np.frombuffer(audio_bytes, dtype=np.float32))
                if audio_tensor.ndim == 1: audio_tensor = audio_tensor.unsqueeze(0) # Add channel dim if mono

                await asyncio.to_thread(torchaudio.save, str(temp_audio_filepath), audio_tensor, sr)
                print(f"ChatterboxTTSTool: Saved generated speech to {temp_audio_filepath}")
                
                # The URL should be relative to how work_dir files are served by Agent Zero's API
                # e.g., if /api/download_work_dir_file takes path relative to work_dir
                # work_dir_path = self.agent.context.get_custom_data("work_dir_path", "work_dir") -> root
                # relative_path = temp_audio_filepath.relative_to(work_dir_path) -> tmp/tts_output/filename.wav
                # This assumes work_dir is accessible directly.
                # If Agent Zero uses /root as work_dir in Docker, path becomes /tmp/tts_output/filename.wav
                # For now, use an absolute-like path that the /api/download_work_dir_file can resolve from base_dir
                # This needs careful handling of Agent Zero's file serving.
                # The download_work_dir_file API takes paths relative to the *root of the work_dir*.
                # If work_dir is /root (docker) or ./work_dir (local), then path should be "tmp/tts_output/..."
                relative_audio_path = f"tmp/tts_output/{temp_audio_filename}"


            except Exception as e:
                print(f"ChatterboxTTSTool: Error saving TTS audio to file: {e}")
                relative_audio_path = None # Indicate save error

            result_details = {
                "sample_rate": sr, 
                "audio_path": relative_audio_path if relative_audio_path else "Error saving audio", # Relative path for API download
                "audio_data_base64_preview": audio_base64[:256] + "..." if audio_base64 else None, # Small preview
                "text_length": len(text),
                "audio_duration_seconds": len(audio_bytes) / (sr * np.dtype(np.float32).itemsize) # Correct duration for float32
            }
            await self._emit_tts_event("generate_speech", "completed", result_details)
            return ToolResponse(message="Speech generated successfully." + (f" Saved to {relative_audio_path}" if relative_audio_path else ""), data=result_details)

        # _convert_voice remains mock for this task
        async def _convert_voice(self, vc_handler: ChatterboxVCHandler, source_audio_path: str, target_voice_path: str) -> ToolResponse:
            await self._emit_tts_event("convert_voice", "starting", {"source": source_audio_path, "target_prompt": target_voice_path})
            sr, audio_bytes = await vc_handler.convert_voice(source_audio_path, target_voice_path)
            audio_base64 = base64.b64encode(audio_bytes[:1024]).decode('utf-8')
            result_details = {"sample_rate": sr, "audio_data_base64_snippet": audio_base64}
            await self._emit_tts_event("convert_voice", "completed", result_details)
            return ToolResponse(message="Voice conversion successful (mock).", data=result_details)
```
    **Note on `ChatterboxTTSHandler._ensure_model_loaded()`:**
    *   This uses a class-level lock and instance variable for the model to ensure it's loaded only once per application instance (across different tool instances if that were to happen, though here tool instantiates handler).
    *   `from_pretrained` is called in a thread to avoid blocking the asyncio loop, as it can be a long-running, CPU/GPU-bound operation.
    *   The `agent.config.get("tts_device", ...)` allows specifying the device in Agent Zero's settings.

**Dependencies/Prerequisites:**
*   Tasks 1-17 completed.
*   `chatterbox-tts` and its dependencies (torch, torchaudio, librosa, s3tokenizer, perth, etc.) are installed and accessible in the Python environment.
*   Chatterbox models downloaded (e.g., via Hugging Face Hub, as handled by `ChatterboxTTS.from_pretrained`).
*   `OPENAI_API_KEY` (if Chatterbox internals or its dependencies like `s3tokenizer` might still try to use it, though Chatterbox TTS itself is local).

**Integration with Agent Zero:**
*   `ChatterboxTTSHandler` now loads and uses the actual `ChatterboxTTS` model.
*   The `generate_speech` method performs real TTS.
*   Audio output is saved to a temporary file within a `tts_output` subdirectory of Agent Zero's `tmp` folder (which is within `work_dir`), and a relative path is returned. This path can be used with an Agent Zero API endpoint (like a modified `download_work_dir_file`) to serve the audio.
*   The tool relies on `agent.config` for device configuration.

**Chatterbox TTS Integration Requirements for this Task:**
*   This task *is* the core implementation of Chatterbox TTS generation.

**Docker Compatibility:**
*   All `chatterbox-tts` dependencies must be in `requirements.txt`.
*   The Docker image needs to be potentially larger due to these dependencies (especially `torch`).
*   If `torch` is used with GPU support, the Docker image must be CUDA-enabled, and the host needs NVIDIA drivers and Docker configured for GPU access.
*   `huggingface_hub` caching directory (`~/.cache/huggingface` by default) should be considered for Docker volumes if models are downloaded at runtime, to persist them across container restarts. Alternatively, models can be baked into the image or mounted from a volume. `ChatterboxTTS.from_pretrained` handles model downloading.

**Summary of Task 19:**
This task implements the core TTS generation functionality of the `ChatterboxTTSTool` by integrating the actual `chatterbox-tts` library. The `ChatterboxTTSHandler` now loads the pre-trained models and uses them to synthesize speech from text. The output audio is saved to a temporary file, and its path is returned, making it accessible for playback or further processing. Voice Conversion (VC) part of the handler remains a placeholder for future implementation.

Please confirm to proceed.Okay, let's proceed with Task 19.

## Task 19: Implement Real Logic for `ChatterboxTTSTool` - Model Loading and Basic TTS Generation

**Focus:**
This task transitions the `ChatterboxTTSTool` from placeholder logic to a functional implementation for the `generate_speech` action. This involves:
1.  Integrating the actual `chatterbox-tts` library.
2.  Implementing model loading for `ChatterboxTTS` within `ChatterboxTTSHandler`.
3.  Implementing the `generate_speech` method in `ChatterboxTTSHandler` to perform actual audio synthesis.
4.  Handling audio data (e.g., saving to a temporary file and returning a path, or returning base64 encoded data).
5.  Ensuring Perth watermarking is applied.

**Assumptions:**
*   The `chatterbox-tts` library and its models are accessible (e.g., via Hugging Face Hub or local paths).
*   Dependencies of `chatterbox-tts` (like `torch`, `torchaudio`, `librosa`, `s3tokenizer`, `resemble-perth`) are managed.

**File Paths and Code Changes:**

1.  **Modify `requirements.txt`:**
    *   Add `chatterbox-tts` and its core dependencies if not already covered by other tools (e.g., `torch`).
    *   The `pyproject.toml` from `chatterbox full code.md` lists:
        ```
        numpy>=1.26.0
        librosa==0.11.0
        s3tokenizer
        torch==2.6.0
        torchaudio==2.6.0
        transformers==4.46.3
        diffusers==0.29.0
        resemble-perth==1.0.1
        conformer==0.3.2
        safetensors==0.5.3
        ```
    *   We need to ensure these (or compatible versions) are in `requirements.txt`.

    ```
    # requirements.txt
    # ... (existing requirements)
    chatterbox-tts # This should pull its own dependencies as defined in its pyproject.toml
    # If chatterbox-tts is not on PyPI, or for local dev:
    # -e ./path/to/chatterbox_local_clone # If installing from a local clone
    # Or list its specific dependencies if installing them manually:
    # numpy>=1.26.0
    # librosa==0.11.0
    # s3tokenizer
    # torch # Ensure version compatibility with other tools
    # torchaudio # Ensure version compatibility
    # transformers
    # diffusers
    # resemble-perth==1.0.1
    # conformer==0.3.2
    # safetensors 
    ```
    For this task, let's assume `pip install chatterbox-tts` (if it were a package) or `pip install -e /path/to/chatterbox_src_dir` would handle its dependencies. We will use the direct import from `chatterbox.tts` as if it's installed.

2.  **Modify `python/agents/tts_agent/chatterbox_handler.py`:**
    *   Replace the mock `ChatterboxTTSHandler` with a real one that loads and uses `chatterbox.tts.ChatterboxTTS`.

    ```python
    # python/agents/tts_agent/chatterbox_handler.py
    import asyncio
    from typing import Dict, Any, Optional, Tuple
    import numpy as np
    import torch
    import librosa # For loading audio prompt if needed by Chatterbox internal
    import tempfile
    import os
    from pathlib import Path

    try:
        from chatterbox.tts import ChatterboxTTS as RealChatterboxTTS, punc_norm
        from chatterbox.models.s3gen import S3GEN_SR # Sample rate from chatterbox
        CHATTERBOX_AVAILABLE = True
    except ImportError:
        print("ChatterboxTTSHandler: chatterbox-tts library not found. TTS tool will not be functional.")
        CHATTERBOX_AVAILABLE = False
        # Placeholder for S3GEN_SR if library not found
        S3GEN_SR = 24000 
        class RealChatterboxTTS: # Placeholder
            def __init__(self, *args, **kwargs): pass
            @classmethod
            def from_pretrained(cls, *args, **kwargs): return cls()
            def generate(self, *args, **kwargs): 
                num_samples = int(S3GEN_SR * 2.0)
                return torch.zeros(1, num_samples, dtype=torch.float32) # Batch dim, samples
            @property
            def sr(self): return S3GEN_SR

    class ChatterboxTTSHandler:
        """
        Handles Text-to-Speech generation using the real Chatterbox library.
        """
        _model_instance: Optional[RealChatterboxTTS] = None
        _model_lock = asyncio.Lock()

        def __init__(self, device: str = "cpu"):
            self.device = device
            self.sr = S3GEN_SR # Sample rate of Chatterbox output
            # Model loading is deferred to ensure it happens in the correct async context / process
            print(f"ChatterboxTTSHandler: Initialized for device: {device}. Model will be loaded on first use.")

        async def _ensure_model_loaded(self):
            async with self._model_lock:
                if not CHATTERBOX_AVAILABLE:
                    raise RuntimeError("Chatterbox library is not installed. TTS functionality is unavailable.")
                if ChatterboxTTSHandler._model_instance is None:
                    print(f"ChatterboxTTSHandler: Loading ChatterboxTTS model for device '{self.device}'...")
                    try:
                        # from_pretrained might be synchronous, wrap if necessary
                        ChatterboxTTSHandler._model_instance = await asyncio.to_thread(
                            RealChatterboxTTS.from_pretrained, device=self.device
                        )
                        print("ChatterboxTTSHandler: Model loaded successfully.")
                    except Exception as e:
                        print(f"ChatterboxTTSHandler: Failed to load ChatterboxTTS model: {e}")
                        import traceback
                        traceback.print_exc()
                        ChatterboxTTSHandler._model_instance = None # Ensure it's None on failure
                        raise # Re-raise exception to signal failure
                
                if ChatterboxTTSHandler._model_instance is None:
                     raise RuntimeError("ChatterboxTTS model could not be loaded.")
            return ChatterboxTTSHandler._model_instance

        async def generate_speech(self, text: str, audio_prompt_path: Optional[str] = None, 
                                  exaggeration: float = 0.5, cfg_weight: float = 0.5, 
                                  temperature: float = 0.8) -> Tuple[int, bytes]:
            """Generates speech using ChatterboxTTS model."""
            if not CHATTERBOX_AVAILABLE:
                 raise RuntimeError("Chatterbox library is not installed. Cannot generate speech.")

            model = await self._ensure_model_loaded()
            
            print(f"ChatterboxTTSHandler: Generating speech for text: '{text[:50]}...' with prompt: {audio_prompt_path}")
            
            # Normalize punctuation as per Chatterbox example
            normalized_text = punc_norm(text)

            # Chatterbox's model.generate is synchronous
            # We need to run it in a thread pool executor to avoid blocking the asyncio loop
            try:
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate,
                    text=normalized_text,
                    audio_prompt_path=audio_prompt_path, # Path to a .wav file
                    exaggeration=exaggeration,
                    cfg_weight=cfg_weight,
                    temperature=temperature
                ) # Returns torch.Tensor of shape (1, num_samples)
            except Exception as e:
                print(f"ChatterboxTTSHandler: Error during model.generate: {e}")
                import traceback
                traceback.print_exc()
                raise

            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0:
                print("ChatterboxTTSHandler: Model generated empty audio.")
                raise ValueError("TTS model generated empty audio.")

            # Assuming batch size of 1 from model.generate output
            wav_tensor = wav_tensor_batched.squeeze(0).cpu() # (num_samples,)
            audio_numpy = wav_tensor.numpy()

            # Watermarking is handled internally by ChatterboxTTS.generate if it uses its own watermarker.
            # The example in chatterbox/tts.py shows:
            #   wav = model.generate(...)
            #   watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
            # So, the `model.generate` in ChatterboxTTS already returns watermarked audio.
            # If `model.generate` does NOT return watermarked audio, we'd do it here:
            # watermarker = perth.PerthImplicitWatermarker() # From chatterbox.tts
            # watermarked_audio_numpy = watermarker.apply_watermark(audio_numpy, sample_rate=self.sr)
            # audio_bytes = watermarked_audio_numpy.astype(np.float32).tobytes()
            
            audio_bytes = audio_numpy.astype(np.float32).tobytes() # Assuming generate() already watermarked
            
            print(f"ChatterboxTTSHandler: Speech generated ({len(audio_bytes)} bytes, SR: {self.sr}).")
            return self.sr, audio_bytes

    # ChatterboxVCHandler would be updated similarly if VC is a priority.
    # For now, focusing on TTS. If VC is needed, it would follow a similar pattern:
    # class ChatterboxVCHandler: ... load VC model, implement convert_voice ...
    # For this task, we'll leave ChatterboxVCHandler as its mock version from Task 10
    # to keep the scope manageable.
    class ChatterboxVCHandler: # Keeping mock from Task 10 for now
        def __init__(self, device: str = "cpu"):
            self.device = device
            self.sr = S3GEN_SR 
            print(f"ChatterboxVCHandler (Mock - VC Not Implemented in this task) initialized on device: {device}")
        
        async def _mock_audio_data(self, duration_seconds: float = 2.0) -> Tuple[int, np.ndarray]:
            num_samples = int(self.sr * duration_seconds)
            audio_data = np.zeros(num_samples, dtype=np.float32)
            return self.sr, audio_data

        async def convert_voice(self, source_audio_path: str, target_voice_path: str) -> Tuple[int, bytes]:
            print(f"ChatterboxVCHandler (Mock): VC from '{source_audio_path}' to target '{target_voice_path}'. NOT IMPLEMENTED.")
            sr, audio_np = await self._mock_audio_data(duration_seconds=3.0) 
            audio_bytes = audio_np.tobytes()
            return sr, audio_bytes
    ```

4.  **Modify `python/tools/chatterbox_tts_tool.py`:**
    *   Ensure it correctly instantiates and calls the updated `ChatterboxTTSHandler`.
    *   The response should now contain real (though potentially temporary) audio data. A common way to return audio is to save it to a temporary file in `tmp/tts_output/` and return the file path, or return base64 encoded data. For AG-UI, a URL to the audio or base64 might be suitable.
    *   We need a mechanism to serve these temporary files if paths are returned. Agent Zero already has `/api/download_work_dir_file` and `/api/image_get`. A similar one for TTS audio might be needed or adapt an existing one. For now, let's return base64.

    ```python
    # python/tools/chatterbox_tts_tool.py
    # ... (imports as before)
    import tempfile # For saving audio temporarily if needed
    from pathlib import Path # For path manipulation

    class ChatterboxTTSTool(Tool):
        _tts_handler_instance: Optional[ChatterboxTTSHandler] = None
        _vc_handler_instance: Optional[ChatterboxVCHandler] = None
        _handler_lock = asyncio.Lock()

        @classmethod
        async def get_tts_handler(cls, device: str) -> ChatterboxTTSHandler:
            async with cls._handler_lock:
                if cls._tts_handler_instance is None:
                    cls._tts_handler_instance = ChatterboxTTSHandler(device=device)
                # Ensure model is loaded within handler if not already
                await cls._tts_handler_instance._ensure_model_loaded()
            return cls._tts_handler_instance

        @classmethod
        async def get_vc_handler(cls, device: str) -> ChatterboxVCHandler: # VC still mock
            async with cls._handler_lock:
                if cls._vc_handler_instance is None:
                    cls._vc_handler_instance = ChatterboxVCHandler(device=device)
            return cls._vc_handler_instance


        def __init__(self, agent, **kwargs):
            super().__init__(agent, name="chatterbox_tts_tool",
                             description="Generates speech from text (TTS) or performs voice conversion (VC) using Chatterbox models.",
                             args_schema=None,
                             **kwargs)
            self.device = agent.config.get("tts_device", agent.config.get("device", "cpu")) # Get device from agent config
            # Handlers will be fetched on demand to ensure model loading happens in async context
            print(f"ChatterboxTTSTool initialized for agent {agent.agent_name}. Handlers will use device: {self.device}")

        # ... (_emit_tts_event method as before)

        async def execute(self, action: str, **kwargs) -> ToolResponse:
            # ... (action routing as before) ...
            try:
                if action == "generate_speech":
                    tts_handler = await self.get_tts_handler(self.device)
                    text = kwargs.get("text")
                    # ... (get other params: audio_prompt_path, exaggeration, etc.)
                    if not text: return ToolResponse("Error: 'text' is required for generate_speech.", error=True)
                    
                    audio_prompt_path = kwargs.get("audio_prompt_path") 
                    exaggeration = float(kwargs.get("exaggeration", 0.5))
                    cfg_weight = float(kwargs.get("cfg_weight", 0.5))
                    temperature = float(kwargs.get("temperature", 0.8))
                    
                    return await self._generate_speech(tts_handler, text, audio_prompt_path, exaggeration, cfg_weight, temperature)
                
                elif action == "convert_voice":
                    vc_handler = await self.get_vc_handler(self.device) # Still mock
                    # ... (get source_audio_path, target_voice_path)
                    source_audio_path = kwargs.get("source_audio_path")
                    target_voice_path = kwargs.get("target_voice_path")
                    if not source_audio_path or not target_voice_path:
                        return ToolResponse("Error: 'source_audio_path' and 'target_voice_path' required.", error=True)
                    return await self._convert_voice(vc_handler, source_audio_path, target_voice_path) # Calls mock
                # ...
            # ... (exception handling as before)
            except RuntimeError as rne: # Catch model loading errors specifically
                error_message = f"ChatterboxTTSTool runtime error (likely model loading) during action '{action}': {str(rne)}"
                print(error_message)
                await self._emit_tts_event(action, "error", {"error": str(rne), "type": "RuntimeError"})
                return ToolResponse(message=error_message, error=True)
            except Exception as e: # General errors
                # ... (as before)
                import traceback
                error_message = f"ChatterboxTTSTool error during action '{action}': {str(e)}\n{traceback.format_exc()}"
                print(error_message)
                await self._emit_tts_event(action, "error", {"error": str(e)})
                return ToolResponse(message=error_message, error=True)


        async def _generate_speech(self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
                                   exaggeration: float, cfg_weight: float, temperature: float) -> ToolResponse:
            await self._emit_tts_event("generate_speech", "starting", {"text_length": len(text), "prompt": bool(audio_prompt_path)})
            
            sr, audio_bytes = await tts_handler.generate_speech(
                text, audio_prompt_path, exaggeration, cfg_weight, temperature
            )
            
            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
            
            # Create a temporary file path for the audio.
            # The UI will need an endpoint to fetch this if it doesn't handle base64 directly.
            # Agent Zero's work_dir is /root in Docker, or ./work_dir locally.
            # Let's use a subdirectory in tmp for TTS outputs for now.
            
            # Ensure tmp/tts_output directory exists
            tts_output_dir = Path(self.agent.context.get_custom_data("work_dir_path", "work_dir")) / "tmp" / "tts_output"
            tts_output_dir.mkdir(parents=True, exist_ok=True) # Agent Zero might need a helper for this

            temp_audio_filename = f"tts_output_{uuid.uuid4()}.wav"
            temp_audio_filepath = tts_output_dir / temp_audio_filename
            
            try:
                # Save the raw bytes as a .wav file using torchaudio or soundfile
                # For raw bytes (assuming float32 PCM), need to construct a WAV header or use a library
                # Torchaudio is a dependency of chatterbox, so it should be available.
                import torchaudio
                # Convert raw bytes back to tensor
                audio_tensor = torch.from_numpy(np.frombuffer(audio_bytes, dtype=np.float32))
                if audio_tensor.ndim == 1: audio_tensor = audio_tensor.unsqueeze(0) # Add channel dim if mono

                await asyncio.to_thread(torchaudio.save, str(temp_audio_filepath), audio_tensor, sr)
                print(f"ChatterboxTTSTool: Saved generated speech to {temp_audio_filepath}")
                
                # The URL should be relative to how work_dir files are served by Agent Zero's API
                # e.g., if /api/download_work_dir_file takes path relative to work_dir
                # work_dir_path = self.agent.context.get_custom_data("work_dir_path", "work_dir") -> root
                # relative_path = temp_audio_filepath.relative_to(work_dir_path) -> tmp/tts_output/filename.wav
                # This assumes work_dir is accessible directly.
                # If Agent Zero uses /root as work_dir in Docker, path becomes /tmp/tts_output/filename.wav
                # For now, use an absolute-like path that the /api/download_work_dir_file can resolve from base_dir
                # This needs careful handling of Agent Zero's file serving.
                # The download_work_dir_file API takes paths relative to the *root of the work_dir*.
                # If work_dir is /root (docker) or ./work_dir (local), then path should be "tmp/tts_output/..."
                relative_audio_path = f"tmp/tts_output/{temp_audio_filename}"


            except Exception as e:
                print(f"ChatterboxTTSTool: Error saving TTS audio to file: {e}")
                relative_audio_path = None # Indicate save error

            result_details = {
                "sample_rate": sr, 
                "audio_path": relative_audio_path if relative_audio_path else "Error saving audio", # Relative path for API download
                "audio_data_base64_preview": audio_base64[:256] + "..." if audio_base64 else None, # Small preview
                "text_length": len(text),
                "audio_duration_seconds": len(audio_bytes) / (sr * np.dtype(np.float32).itemsize) # Correct duration for float32
            }
            await self._emit_tts_event("generate_speech", "completed", result_details)
            return ToolResponse(message="Speech generated successfully." + (f" Saved to {relative_audio_path}" if relative_audio_path else ""), data=result_details)

        # _convert_voice remains mock for this task
        async def _convert_voice(self, vc_handler: ChatterboxVCHandler, source_audio_path: str, target_voice_path: str) -> ToolResponse:
            await self._emit_tts_event("convert_voice", "starting", {"source": source_audio_path, "target_prompt": target_voice_path})
            sr, audio_bytes = await vc_handler.convert_voice(source_audio_path, target_voice_path)
            audio_base64 = base64.b64encode(audio_bytes[:1024]).decode('utf-8')
            result_details = {"sample_rate": sr, "audio_data_base64_snippet": audio_base64}
            await self._emit_tts_event("convert_voice", "completed", result_details)
            return ToolResponse(message="Voice conversion successful (mock).", data=result_details)

    ```
    **Note on `ChatterboxTTSHandler._ensure_model_loaded()`:**
    *   This uses a class-level lock and instance variable for the model to ensure it's loaded only once per application instance (across different tool instances if that were to happen, though here tool instantiates handler).
    *   `from_pretrained` is called in a thread to avoid blocking the asyncio loop, as it can be a long-running, CPU/GPU-bound operation.
    *   The `agent.config.get("tts_device", ...)` allows specifying the device in Agent Zero's settings.

**Dependencies/Prerequisites:**
*   Tasks 1-17 completed.
*   `chatterbox-tts` and its dependencies (torch, torchaudio, librosa, s3tokenizer, perth, etc.) are installed and accessible in the Python environment.
*   Chatterbox models downloaded (e.g., via Hugging Face Hub, as handled by `ChatterboxTTS.from_pretrained`).
*   `OPENAI_API_KEY` (if Chatterbox internals or its dependencies like `s3tokenizer` might still try to use it, though Chatterbox TTS itself is local).

**Integration with Agent Zero:**
*   `ChatterboxTTSHandler` now loads and uses the actual `ChatterboxTTS` model.
*   The `generate_speech` method performs real TTS.
*   Audio output is saved to a temporary file within a `tts_output` subdirectory of Agent Zero's `tmp` folder (which is within `work_dir`), and a relative path is returned. This path can be used with an Agent Zero API endpoint (like a modified `download_work_dir_file`) to serve the audio.
*   The tool relies on `agent.config` for device configuration.

**Chatterbox TTS Integration Requirements for this Task:**
*   This task *is* the core implementation of Chatterbox TTS generation.

**Docker Compatibility:**
*   All `chatterbox-tts` dependencies must be in `requirements.txt`.
*   The Docker image needs to be potentially larger due to these dependencies (especially `torch`).
*   If `torch` is used with GPU support, the Docker image must be CUDA-enabled, and the host needs NVIDIA drivers and Docker configured for GPU access.
*   `huggingface_hub` caching directory (`~/.cache/huggingface` by default) should be considered for Docker volumes if models are downloaded at runtime, to persist them across container restarts. Alternatively, models can be baked into the image or mounted from a volume. `ChatterboxTTS.from_pretrained` handles model downloading.

**Summary of Task 19:**
This task implements the core TTS generation functionality of the `ChatterboxTTSTool` by integrating the actual `chatterbox-tts` library. The `ChatterboxTTSHandler` now loads the pre-trained models and uses them to synthesize speech from text. The output audio is saved to a temporary file, and its path is returned, making it accessible for playback or further processing. Voice Conversion (VC) part of the handler remains a placeholder for future implementation.

Please confirm to proceed.