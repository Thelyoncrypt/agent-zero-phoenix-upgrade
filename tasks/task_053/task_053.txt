## **Task 53: `ChatterboxTTSTool` - Audio Streaming Output (Conceptual for AG-UI)**

**Focus:**
This task focuses on the conceptual design and backend modifications required to support streaming audio output from the `ChatterboxTTSTool` over WebSockets, aligning with advanced AG-UI protocol capabilities. Actually implementing chunked TTS generation within Chatterbox and the client-side audio reconstruction is highly complex and likely beyond a single task if Chatterbox doesn't natively support streaming its output.

**Therefore, this task will focus on:**
1.  Defining a new `StreamEventType` (e.g., `AUDIO_CHUNK` or `TTS_STREAM_CHUNK`).
2.  Modifying `ChatterboxTTSHandler` to (conceptually) yield audio chunks if the underlying `ChatterboxTTS.generate` method could be adapted or wrapped for streaming. For now, if it only produces a full audio buffer, we'll simulate chunking this buffer.
3.  Updating `ChatterboxTTSTool` to emit these chunks via `StreamProtocolTool`.
4.  The tool will still save the full audio file as a fallback or for clients not supporting audio streaming.

**File Paths and Code Changes:**

1.  **Modify `python/tools/stream_protocol_tool.py` (or a central `events.py`):**
    *   Add new `StreamEventType` for audio chunks.

    ```python
# python/tools/stream_protocol_tool.py or a shared events.py
    class StreamEventType(Enum):
        # ... (existing event types)
        TEXT_MESSAGE_CONTENT = "text_message_content"
        # ...
        AUDIO_CHUNK = "audio_chunk" # New event for streaming audio
        TTS_STREAM_START = "tts_stream_start" # Signals start of TTS audio stream
        TTS_STREAM_END = "tts_stream_end"   # Signals end of TTS audio stream
```

2.  **Modify `python/agents/tts_agent/chatterbox_handler.py` (`ChatterboxTTSHandler`):**
    *   Modify `generate_speech` to be an `async def` that can `yield` audio chunks.
    *   If `ChatterboxTTS.generate` is blocking and returns full audio, we'll simulate chunking after generation. True streaming requires the TTS model itself to support yielding chunks during synthesis.

    ```python
# python/agents/tts_agent/chatterbox_handler.py
    import asyncio
    from typing import Dict, Any, Optional, Tuple, AsyncGenerator
    import numpy as np
    import torch
    # ... (other imports as in Task 30)

    # Assume S3GEN_SR and RealChatterboxTTS/CHATTERBOX_AVAILABLE are defined as in Task 30

    class ChatterboxTTSHandler:
        # ... (__init__, _ensure_model_loaded as in Task 30)

        async def generate_speech_stream(
            self, 
            text: str, 
            audio_prompt_path: Optional[str] = None, 
            exaggeration: float = 0.5, 
            cfg_weight: float = 0.5, 
            temperature: float = 0.8,
            chunk_duration_ms: int = 100 # Duration of each audio chunk in milliseconds
        ) -> AsyncGenerator[Tuple[bytes, bool], None]: # Yields (audio_chunk_bytes, is_last_chunk)
            """
            Generates speech and yields audio chunks.
            If Chatterbox model doesn't support native streaming, this simulates it.
            """
            if not CHATTERBOX_AVAILABLE:
                raise RuntimeError("Chatterbox library not installed. Cannot generate speech.")

            model = await self._ensure_model_loaded()
            logger.info(f"ChatterboxTTSHandler: Generating speech stream for text: '{text[:50]}...'")
            normalized_text = punc_norm(text) # Assuming punc_norm is available

            try:
                # Simulate a blocking call to the TTS model
                # In a real streaming TTS, model.generate itself would be an async generator or accept a callback.
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate,
                    text=normalized_text, audio_prompt_path=audio_prompt_path,
                    exaggeration=exaggeration, cfg_weight=cfg_weight, temperature=temperature
                )
            except Exception as e:
                logger.error(f"ChatterboxTTSHandler: Error during model.generate for streaming: {e}", exc_info=True)
                raise # Re-raise to be caught by the tool

            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0:
                logger.warning("ChatterboxTTSHandler: Model generated empty audio for streaming.")
                yield (b'', True) # Empty bytes, is_last_chunk = True
                return

            wav_tensor = wav_tensor_batched.squeeze(0).cpu() # (num_samples,)
            audio_numpy = wav_tensor.numpy().astype(np.float32) # Ensure float32

            # Watermarking should ideally happen on the full audio or be stream-compatible.
            # For simplicity in this simulation, assume generate() already watermarked.
            # If not:
            # watermarker = perth.PerthImplicitWatermarker()
            # audio_numpy = watermarker.apply_watermark(audio_numpy, sample_rate=model.sr)

            samples_per_chunk = int(model.sr * (chunk_duration_ms / 1000.0))
            total_samples = audio_numpy.shape[0]
            num_chunks = (total_samples + samples_per_chunk - 1) // samples_per_chunk

            logger.info(f"ChatterboxTTSHandler: Streaming {total_samples} samples in {num_chunks} chunks ({samples_per_chunk} samples/chunk).")

            for i in range(num_chunks):
                start_sample = i * samples_per_chunk
                end_sample = min((i + 1) * samples_per_chunk, total_samples)
                chunk_numpy = audio_numpy[start_sample:end_sample]
                chunk_bytes = chunk_numpy.tobytes()
                is_last = (i == num_chunks - 1)
                
                logger.debug(f"ChatterboxTTSHandler: Yielding chunk {i+1}/{num_chunks}, {len(chunk_bytes)} bytes, is_last={is_last}")
                yield (chunk_bytes, is_last)
                if not is_last:
                    await asyncio.sleep(chunk_duration_ms / 1000.0 * 0.8) # Simulate real-time generation pace slightly faster than chunk duration
            logger.info(f"ChatterboxTTSHandler: Finished streaming audio chunks.")

        # Keep the original generate_speech method for non-streaming cases (returns full audio bytes)
        async def generate_speech_full(self, text: str, audio_prompt_path: Optional[str] = None, 
                                  exaggeration: float = 0.5, cfg_weight: float = 0.5, 
                                  temperature: float = 0.8) -> Tuple[int, bytes]:
            # This is essentially the body of generate_speech from Task 30
            if not CHATTERBOX_AVAILABLE: raise RuntimeError("Chatterbox library not installed.")
            model = await self._ensure_model_loaded()
            normalized_text = punc_norm(text)
            try:
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate, text=normalized_text, audio_prompt_path=audio_prompt_path,
                    exaggeration=exaggeration, cfg_weight=cfg_weight, temperature=temperature
                )
            except Exception as e: logger.error(f"TTSHandler: Error model.generate: {e}", exc_info=True); raise
            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0: raise ValueError("TTS empty audio.")
            audio_numpy = wav_tensor_batched.squeeze(0).cpu().numpy().astype(np.float32)
            # Assume watermarking by model.generate
            return model.sr, audio_numpy.tobytes()

    # ChatterboxVCHandler would need similar streaming adaptation if VC streaming is desired.
    # For now, VCHandler remains non-streaming.
```

3.  **Modify `python/tools/chatterbox_tts_tool.py`:**
    *   Add a new parameter to `generate_speech` action, e.g., `stream_audio: bool = False`.
    *   If `stream_audio` is true, `_generate_speech` will call the new `tts_handler.generate_speech_stream` and emit `AUDIO_CHUNK` events.
    *   It will also emit `TTS_STREAM_START` and `TTS_STREAM_END`.
    *   The final `ToolResponse` might not contain the full audio data if streamed, but rather a confirmation or metadata.

    ```python
# python/tools/chatterbox_tts_tool.py
    # ... (imports as in Task 31, add AsyncGenerator)
    from typing import Dict, Any, Optional, AsyncGenerator 
    from python.helpers.files import get_tts_output_dir_abs, get_tts_output_web_path # Keep for non-streaming

    class ChatterboxTTSTool(Tool):
        # ... (__init__, _get_tts_handler, _get_vc_handler, _emit_tts_event as before)

        async def execute(self, action: str, **kwargs) -> ToolResponse:
            # ...
            try:
                if action == "generate_speech":
                    tts_handler = await self.get_tts_handler(self.device)
                    text = kwargs.get("text")
                    if not text: return ToolResponse("Error: 'text' is required.", error=True)
                    
                    stream_audio = kwargs.get("stream_audio", False) # New parameter
                    chunk_duration_ms = int(kwargs.get("chunk_duration_ms", 100))

                    audio_prompt_path = kwargs.get("audio_prompt_path")
                    exaggeration = float(kwargs.get("exaggeration", 0.5))
                    cfg_weight = float(kwargs.get("cfg_weight", 0.5))
                    temperature = float(kwargs.get("temperature", 0.8))
                    
                    if stream_audio:
                        return await self._generate_speech_streaming(
                            tts_handler, text, audio_prompt_path, exaggeration, 
                            cfg_weight, temperature, chunk_duration_ms
                        )
                    else:
                        return await self._generate_speech_full_file( # Renamed old method
                            tts_handler, text, audio_prompt_path, exaggeration, 
                            cfg_weight, temperature
                        )
                # ... (convert_voice as before)
            # ... (exception handling as before)
            except Exception as e: # General error handling
                # ... (as before)
                pass

        async def _generate_speech_streaming(
            self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
            exaggeration: float, cfg_weight: float, temperature: float, chunk_duration_ms: int
        ) -> ToolResponse:
            
            stream_id = f"tts_stream_{uuid.uuid4()}"
            await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_START, 
                {"text": text, "stream_id": stream_id, "sample_rate": tts_handler.sr, 
                 "encoding": "pcm_f32le", "channels": 1}) # PCM float32 little-endian

            chunk_count = 0
            full_audio_bytes_for_saving = bytearray()

            try:
                async for audio_chunk_bytes, is_last_chunk in tts_handler.generate_speech_stream(
                    text, audio_prompt_path, exaggeration, cfg_weight, temperature, chunk_duration_ms
                ):
                    chunk_count += 1
                    full_audio_bytes_for_saving.extend(audio_chunk_bytes)
                    
                    # Encode chunk to base64 for JSON payload if required by AG-UI spec for AUDIO_CHUNK,
                    # or client might support raw bytes over WebSocket if protocol allows.
                    # AG-UI spec likely implies JSON, so base64 is safer.
                    chunk_base64 = base64.b64encode(audio_chunk_bytes).decode('utf-8')
                    
                    await self.agent._emit_stream_event(StreamEventType.AUDIO_CHUNK, 
                        {"stream_id": stream_id, "chunk_index": chunk_count -1, 
                         "data_base64": chunk_base64, "is_last": is_last_chunk})
                    
                    if is_last_chunk:
                        break
                
                await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_END, {"stream_id": stream_id, "total_chunks": chunk_count})
                
                # Save the full audio as a fallback or for archival, even if streamed
                saved_path = await self._save_audio_and_get_path(bytes(full_audio_bytes_for_saving), tts_handler.sr, f"tts_stream_{stream_id}")
                
                return ToolResponse(
                    message=f"Speech streaming initiated with ID {stream_id}. Full audio saved to {saved_path if saved_path else 'N/A'}.", 
                    data={"stream_id": stream_id, "total_chunks": chunk_count, "full_audio_path": saved_path}
                )

            except Exception as e:
                logger.error(f"ChatterboxTTSTool: Error during speech streaming: {e}", exc_info=True)
                await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_END, {"stream_id": stream_id, "error": str(e)})
                await self.agent._emit_stream_event(StreamEventType.ERROR_EVENT, {"source_tool": self.name, "action": "generate_speech_streaming", "error": str(e)})
                return ToolResponse(message=f"Speech streaming failed: {str(e)}", error=True)


        async def _generate_speech_full_file( # Old _generate_speech method renamed
            self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
            exaggeration: float, cfg_weight: float, temperature: float
        ) -> ToolResponse:
            # This is the content of the previous _generate_speech method from Task 31
            # which saves to file and returns path.
            await self._emit_tts_event("generate_speech_full", "starting", {"text_length": len(text), "prompt": bool(audio_prompt_path)})
            try:
                sr, audio_bytes = await tts_handler.generate_speech_full( # Call the full generation method
                    text, audio_prompt_path, exaggeration, cfg_weight, temperature
                )
            except Exception as e:
                await self._emit_tts_event("generate_speech_full", "error", {"error": str(e)})
                return ToolResponse(message=f"Speech generation failed: {str(e)}", error=True)

            audio_web_path = await self._save_audio_and_get_path(audio_bytes, sr, "tts_full")
            # ... (rest of logic from Task 31's _generate_speech) ...
            result_details = {"sample_rate": sr, "audio_path": audio_web_path or "Error saving", ...}
            status = "completed" if audio_web_path else "failed_to_save"
            await self._emit_tts_event("generate_speech_full", status, result_details)
            if audio_web_path: return ToolResponse(f"Full speech saved: {audio_web_path}", data=result_details)
            else: return ToolResponse("Speech generated but failed to save.", data=result_details, error=True)
            
        # _save_audio_and_get_path helper remains the same
```

4.  **Update `prompts/default/agent.system.tools.md`:**
    *   Add `stream_audio` and `chunk_duration_ms` parameters to `chatterbox_tts_tool`'s `generate_speech` action.

    ```markdown
# prompts/default/agent.system.tools.md
    # ... (chatterbox_tts_tool description)
    #   For "generate_speech":
    #     text: string - The text to synthesize.
    #     audio_prompt_path: string - (Optional) Path (relative to work_dir) to a .wav file for voice cloning.
    #     exaggeration: float - (Optional, default 0.5) Emotion/intensity control.
    #     cfg_weight: float - (Optional, default 0.5) Pacing/adherence control.
    #     temperature: float - (Optional, default 0.8) Sampling temperature.
    #     stream_audio: boolean - (Optional, default False) If true, audio will be streamed in chunks. If false, an audio_path to the full file is returned.
    #     chunk_duration_ms: int - (Optional, default 100) Duration of each audio chunk in milliseconds if streaming.
    # Example for streaming TTS:
    # {
    #   "tool_name": "chatterbox_tts_tool",
    #   "tool_args": { 
    #     "action": "generate_speech", 
    #     "text": "Hello, this is a streamed audio response.", 
    #     "stream_audio": true,
    #     "chunk_duration_ms": 150
    #   }
    # }
    # Output: If streaming, emits AUDIO_CHUNK events. ToolResponse gives stream_id. If not streaming, data contains 'audio_path'.
```

**Dependencies/Prerequisites:**
*   Tasks 1-52 completed.
*   `ChatterboxTTSHandler` (Task 19, 30) correctly loads and uses the Chatterbox model.
*   `StreamProtocolTool` and `Agent._emit_stream_event` are fully functional for event emission over WebSockets.

**Integration with Agent Zero:**
*   `ChatterboxTTSHandler` is modified to include a `generate_speech_stream` async generator method that (simulates) yielding audio chunks.
*   `ChatterboxTTSTool`'s `generate_speech` action can now optionally use this streaming method.
*   When streaming, the tool emits `TTS_STREAM_START`, multiple `AUDIO_CHUNK` events (with base64 encoded audio data), and `TTS_STREAM_END` via the agent's `_emit_stream_event` mechanism.
*   A full audio file is still saved as a fallback or for archival.

**Chatterbox TTS Integration Requirements for this Task:**
*   This task directly implements audio streaming for the Chatterbox TTS integration. The quality of streaming (low latency, smooth playback) would heavily depend on the underlying Chatterbox model's ability to do true incremental synthesis. Our simulation by chunking a fully generated audio is a first step.

**Docker Compatibility:**
*   No new Python package dependencies.
*   The WebSocket server setup (Task 11) must be robust to handle potentially frequent `AUDIO_CHUNK` messages.

**Summary of Task 53:**
This task introduces audio streaming capabilities to the `ChatterboxTTSTool`. The `ChatterboxTTSHandler` now has a method to (simulate) yielding audio in chunks. The tool, when requested, uses this method to emit a series of `AUDIO_CHUNK` events, allowing an AG-UI compliant frontend to play back TTS audio incrementally. This enhances user experience for longer speech synthesis by providing faster first-audio and continuous playback. Full audio is still saved for fallback.

Please confirm to proceed.Okay, let's proceed with **Task 53: `ChatterboxTTSTool` - Audio Streaming Output (Conceptual for AG-UI)**

**Focus:**
This task focuses on the conceptual design and backend modifications required to support streaming audio output from the `ChatterboxTTSTool` over WebSockets, aligning with advanced AG-UI protocol capabilities. Actually implementing chunked TTS generation within Chatterbox and the client-side audio reconstruction is highly complex and likely beyond a single task if Chatterbox doesn't natively support streaming its output.

**Therefore, this task will focus on:**
1.  Defining a new `StreamEventType` (e.g., `AUDIO_CHUNK` or `TTS_STREAM_CHUNK`).
2.  Modifying `ChatterboxTTSHandler` to (conceptually) yield audio chunks if the underlying `ChatterboxTTS.generate` method could be adapted or wrapped for streaming. For now, if it only produces a full audio buffer, we'll simulate chunking this buffer.
3.  Updating `ChatterboxTTSTool` to emit these chunks via `StreamProtocolTool`.
4.  The tool will still save the full audio file as a fallback or for clients not supporting audio streaming.

**File Paths and Code Changes:**

1.  **Modify `python/tools/stream_protocol_tool.py` (or a central `events.py`):**
    *   Add new `StreamEventType` for audio chunks.

    ```python
    # python/tools/stream_protocol_tool.py or a shared events.py
    class StreamEventType(Enum):
        # ... (existing event types)
        TEXT_MESSAGE_CONTENT = "text_message_content"
        # ...
        AUDIO_CHUNK = "audio_chunk" # New event for streaming audio
        TTS_STREAM_START = "tts_stream_start" # Signals start of TTS audio stream
        TTS_STREAM_END = "tts_stream_end"   # Signals end of TTS audio stream
    ```

2.  **Modify `python/agents/tts_agent/chatterbox_handler.py` (`ChatterboxTTSHandler`):**
    *   Modify `generate_speech` to be an `async def` that can `yield` audio chunks.
    *   If `ChatterboxTTS.generate` is blocking and returns full audio, we'll simulate chunking after generation. True streaming requires the TTS model itself to support yielding chunks during synthesis.

    ```python
    # python/agents/tts_agent/chatterbox_handler.py
    import asyncio
    from typing import Dict, Any, Optional, Tuple, AsyncGenerator
    import numpy as np
    import torch
    # ... (other imports as in Task 30)

    # Assume S3GEN_SR and RealChatterboxTTS/CHATTERBOX_AVAILABLE are defined as in Task 30

    class ChatterboxTTSHandler:
        # ... (__init__, _ensure_model_loaded as in Task 30)

        async def generate_speech_stream(
            self, 
            text: str, 
            audio_prompt_path: Optional[str] = None, 
            exaggeration: float = 0.5, 
            cfg_weight: float = 0.5, 
            temperature: float = 0.8,
            chunk_duration_ms: int = 100 # Duration of each audio chunk in milliseconds
        ) -> AsyncGenerator[Tuple[bytes, bool], None]: # Yields (audio_chunk_bytes, is_last_chunk)
            """
            Generates speech and yields audio chunks.
            If Chatterbox model doesn't support native streaming, this simulates it.
            """
            if not CHATTERBOX_AVAILABLE:
                raise RuntimeError("Chatterbox library not installed. Cannot generate speech.")

            model = await self._ensure_model_loaded()
            logger.info(f"ChatterboxTTSHandler: Generating speech stream for text: '{text[:50]}...'")
            normalized_text = punc_norm(text) # Assuming punc_norm is available

            try:
                # Simulate a blocking call to the TTS model
                # In a real streaming TTS, model.generate itself would be an async generator or accept a callback.
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate,
                    text=normalized_text, audio_prompt_path=audio_prompt_path,
                    exaggeration=exaggeration, cfg_weight=cfg_weight, temperature=temperature
                )
            except Exception as e:
                logger.error(f"ChatterboxTTSHandler: Error during model.generate for streaming: {e}", exc_info=True)
                raise # Re-raise to be caught by the tool

            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0:
                logger.warning("ChatterboxTTSHandler: Model generated empty audio for streaming.")
                yield (b'', True) # Empty bytes, is_last_chunk = True
                return

            wav_tensor = wav_tensor_batched.squeeze(0).cpu() # (num_samples,)
            audio_numpy = wav_tensor.numpy().astype(np.float32) # Ensure float32

            # Watermarking should ideally happen on the full audio or be stream-compatible.
            # For simplicity in this simulation, assume generate() already watermarked.
            # If not:
            # watermarker = perth.PerthImplicitWatermarker()
            # audio_numpy = watermarker.apply_watermark(audio_numpy, sample_rate=model.sr)

            samples_per_chunk = int(model.sr * (chunk_duration_ms / 1000.0))
            total_samples = audio_numpy.shape[0]
            num_chunks = (total_samples + samples_per_chunk - 1) // samples_per_chunk

            logger.info(f"ChatterboxTTSHandler: Streaming {total_samples} samples in {num_chunks} chunks ({samples_per_chunk} samples/chunk).")

            for i in range(num_chunks):
                start_sample = i * samples_per_chunk
                end_sample = min((i + 1) * samples_per_chunk, total_samples)
                chunk_numpy = audio_numpy[start_sample:end_sample]
                chunk_bytes = chunk_numpy.tobytes()
                is_last = (i == num_chunks - 1)
                
                logger.debug(f"ChatterboxTTSHandler: Yielding chunk {i+1}/{num_chunks}, {len(chunk_bytes)} bytes, is_last={is_last}")
                yield (chunk_bytes, is_last)
                if not is_last:
                    await asyncio.sleep(chunk_duration_ms / 1000.0 * 0.8) # Simulate real-time generation pace slightly faster than chunk duration
            logger.info(f"ChatterboxTTSHandler: Finished streaming audio chunks.")

        # Keep the original generate_speech method for non-streaming cases (returns full audio bytes)
        async def generate_speech_full(self, text: str, audio_prompt_path: Optional[str] = None, 
                                  exaggeration: float = 0.5, cfg_weight: float = 0.5, 
                                  temperature: float = 0.8) -> Tuple[int, bytes]:
            # This is essentially the body of generate_speech from Task 30
            if not CHATTERBOX_AVAILABLE: raise RuntimeError("Chatterbox library not installed.")
            model = await self._ensure_model_loaded()
            normalized_text = punc_norm(text)
            try:
                wav_tensor_batched = await asyncio.to_thread(
                    model.generate, text=normalized_text, audio_prompt_path=audio_prompt_path,
                    exaggeration=exaggeration, cfg_weight=cfg_weight, temperature=temperature
                )
            except Exception as e: logger.error(f"TTSHandler: Error model.generate: {e}", exc_info=True); raise
            if wav_tensor_batched is None or wav_tensor_batched.numel() == 0: raise ValueError("TTS empty audio.")
            audio_numpy = wav_tensor_batched.squeeze(0).cpu().numpy().astype(np.float32)
            # Assume watermarking by model.generate
            return model.sr, audio_numpy.tobytes()

    # ChatterboxVCHandler would need similar streaming adaptation if VC streaming is desired.
    # For now, VCHandler remains non-streaming.
    ```

3.  **Modify `python/tools/chatterbox_tts_tool.py`:**
    *   Add a new parameter to `generate_speech` action, e.g., `stream_audio: bool = False`.
    *   If `stream_audio` is true, `_generate_speech` will call the new `tts_handler.generate_speech_stream` and emit `AUDIO_CHUNK` events.
    *   It will also emit `TTS_STREAM_START` and `TTS_STREAM_END`.
    *   The final `ToolResponse` might not contain the full audio data if streamed, but rather a confirmation or metadata.

    ```python
    # python/tools/chatterbox_tts_tool.py
    # ... (imports as in Task 31, add AsyncGenerator)
    from typing import Dict, Any, Optional, AsyncGenerator 
    from python.helpers.files import get_tts_output_dir_abs, get_tts_output_web_path # Keep for non-streaming

    class ChatterboxTTSTool(Tool):
        # ... (__init__, _get_tts_handler, _get_vc_handler, _emit_tts_event as before)

        async def execute(self, action: str, **kwargs) -> ToolResponse:
            # ...
            try:
                if action == "generate_speech":
                    tts_handler = await self.get_tts_handler(self.device)
                    text = kwargs.get("text")
                    if not text: return ToolResponse("Error: 'text' is required.", error=True)
                    
                    stream_audio = kwargs.get("stream_audio", False) # New parameter
                    chunk_duration_ms = int(kwargs.get("chunk_duration_ms", 100))

                    audio_prompt_path = kwargs.get("audio_prompt_path")
                    exaggeration = float(kwargs.get("exaggeration", 0.5))
                    cfg_weight = float(kwargs.get("cfg_weight", 0.5))
                    temperature = float(kwargs.get("temperature", 0.8))
                    
                    if stream_audio:
                        return await self._generate_speech_streaming(
                            tts_handler, text, audio_prompt_path, exaggeration, 
                            cfg_weight, temperature, chunk_duration_ms
                        )
                    else:
                        return await self._generate_speech_full_file( # Renamed old method
                            tts_handler, text, audio_prompt_path, exaggeration, 
                            cfg_weight, temperature
                        )
                # ... (convert_voice as before)
            # ... (exception handling as before)
            except Exception as e: # General error handling
                # ... (as before)
                pass

        async def _generate_speech_streaming(
            self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
            exaggeration: float, cfg_weight: float, temperature: float, chunk_duration_ms: int
        ) -> ToolResponse:
            
            stream_id = f"tts_stream_{uuid.uuid4()}"
            await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_START, 
                {"text": text, "stream_id": stream_id, "sample_rate": tts_handler.sr, 
                 "encoding": "pcm_f32le", "channels": 1}) # PCM float32 little-endian

            chunk_count = 0
            full_audio_bytes_for_saving = bytearray()

            try:
                async for audio_chunk_bytes, is_last_chunk in tts_handler.generate_speech_stream(
                    text, audio_prompt_path, exaggeration, cfg_weight, temperature, chunk_duration_ms
                ):
                    chunk_count += 1
                    full_audio_bytes_for_saving.extend(audio_chunk_bytes)
                    
                    # Encode chunk to base64 for JSON payload if required by AG-UI spec for AUDIO_CHUNK,
                    # or client might support raw bytes over WebSocket if protocol allows.
                    # AG-UI spec likely implies JSON, so base64 is safer.
                    chunk_base64 = base64.b64encode(audio_chunk_bytes).decode('utf-8')
                    
                    await self.agent._emit_stream_event(StreamEventType.AUDIO_CHUNK, 
                        {"stream_id": stream_id, "chunk_index": chunk_count -1, 
                         "data_base64": chunk_base64, "is_last": is_last_chunk})
                    
                    if is_last_chunk:
                        break
                
                await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_END, {"stream_id": stream_id, "total_chunks": chunk_count})
                
                # Save the full audio as a fallback or for archival, even if streamed
                saved_path = await self._save_audio_and_get_path(bytes(full_audio_bytes_for_saving), tts_handler.sr, f"tts_stream_{stream_id}")
                
                return ToolResponse(
                    message=f"Speech streaming initiated with ID {stream_id}. Full audio saved to {saved_path if saved_path else 'N/A'}.", 
                    data={"stream_id": stream_id, "total_chunks": chunk_count, "full_audio_path": saved_path}
                )

            except Exception as e:
                logger.error(f"ChatterboxTTSTool: Error during speech streaming: {e}", exc_info=True)
                await self.agent._emit_stream_event(StreamEventType.TTS_STREAM_END, {"stream_id": stream_id, "error": str(e)})
                await self.agent._emit_stream_event(StreamEventType.ERROR_EVENT, {"source_tool": self.name, "action": "generate_speech_streaming", "error": str(e)})
                return ToolResponse(message=f"Speech streaming failed: {str(e)}", error=True)


        async def _generate_speech_full_file( # Old _generate_speech method renamed
            self, tts_handler: ChatterboxTTSHandler, text: str, audio_prompt_path: Optional[str], 
            exaggeration: float, cfg_weight: float, temperature: float
        ) -> ToolResponse:
            # This is the content of the previous _generate_speech method from Task 31
            # which saves to file and returns path.
            await self._emit_tts_event("generate_speech_full", "starting", {"text_length": len(text), "prompt": bool(audio_prompt_path)})
            try:
                sr, audio_bytes = await tts_handler.generate_speech_full( # Call the full generation method
                    text, audio_prompt_path, exaggeration, cfg_weight, temperature
                )
            except Exception as e:
                await self._emit_tts_event("generate_speech_full", "error", {"error": str(e)})
                return ToolResponse(message=f"Speech generation failed: {str(e)}", error=True)

            audio_web_path = await self._save_audio_and_get_path(audio_bytes, sr, "tts_full")
            # ... (rest of logic from Task 31's _generate_speech) ...
            result_details = {"sample_rate": sr, "audio_path": audio_web_path or "Error saving", ...}
            status = "completed" if audio_web_path else "failed_to_save"
            await self._emit_tts_event("generate_speech_full", status, result_details)
            if audio_web_path: return ToolResponse(f"Full speech saved: {audio_web_path}", data=result_details)
            else: return ToolResponse("Speech generated but failed to save.", data=result_details, error=True)
            
        # _save_audio_and_get_path helper remains the same
    ```

4.  **Update `prompts/default/agent.system.tools.md`:**
    *   Add `stream_audio` and `chunk_duration_ms` parameters to `chatterbox_tts_tool`'s `generate_speech` action.

    ```markdown
    # prompts/default/agent.system.tools.md
    # ... (chatterbox_tts_tool description)
    #   For "generate_speech":
    #     text: string - The text to synthesize.
    #     audio_prompt_path: string - (Optional) Path (relative to work_dir) to a .wav file for voice cloning.
    #     exaggeration: float - (Optional, default 0.5) Emotion/intensity control.
    #     cfg_weight: float - (Optional, default 0.5) Pacing/adherence control.
    #     temperature: float - (Optional, default 0.8) Sampling temperature.
    #     stream_audio: boolean - (Optional, default False) If true, audio will be streamed in chunks. If false, an audio_path to the full file is returned.
    #     chunk_duration_ms: int - (Optional, default 100) Duration of each audio chunk in milliseconds if streaming.
    # Example for streaming TTS:
    # {
    #   "tool_name": "chatterbox_tts_tool",
    #   "tool_args": { 
    #     "action": "generate_speech", 
    #     "text": "Hello, this is a streamed audio response.", 
    #     "stream_audio": true,
    #     "chunk_duration_ms": 150
    #   }
    # }
    # Output: If streaming, emits AUDIO_CHUNK events. ToolResponse gives stream_id. If not streaming, data contains 'audio_path'.
    ```

**Dependencies/Prerequisites:**
*   Tasks 1-52 completed.
*   `ChatterboxTTSHandler` (Task 19, 30) correctly loads and uses the Chatterbox model.
*   `StreamProtocolTool` and `Agent._emit_stream_event` are fully functional for event emission over WebSockets.

**Integration with Agent Zero:**
*   `ChatterboxTTSHandler` is modified to include a `generate_speech_stream` async generator method that (simulates) yielding audio chunks.
*   `ChatterboxTTSTool`'s `generate_speech` action can now optionally use this streaming method.
*   When streaming, the tool emits `TTS_STREAM_START`, multiple `AUDIO_CHUNK` events (with base64 encoded audio data), and `TTS_STREAM_END` via the agent's `_emit_stream_event` mechanism.
*   A full audio file is still saved as a fallback or for archival.

**Chatterbox TTS Integration Requirements for this Task:**
*   This task directly implements audio streaming for the Chatterbox TTS integration. The quality of streaming (low latency, smooth playback) would heavily depend on the underlying Chatterbox model's ability to do true incremental synthesis. Our simulation by chunking a fully generated audio is a first step.

**Docker Compatibility:**
*   No new Python package dependencies.
*   The WebSocket server setup (Task 11) must be robust to handle potentially frequent `AUDIO_CHUNK` messages.

**Summary of Task 53:**
This task introduces audio streaming capabilities to the `ChatterboxTTSTool`. The `ChatterboxTTSHandler` now has a method to (simulate) yielding audio in chunks. The tool, when requested, uses this method to emit a series of `AUDIO_CHUNK` events, allowing an AG-UI compliant frontend to play back TTS audio incrementally. This enhances user experience for longer speech synthesis by providing faster first-audio and continuous playback. Full audio is still saved for fallback.

Please confirm to proceed.