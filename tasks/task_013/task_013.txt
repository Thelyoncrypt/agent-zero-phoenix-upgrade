## Task 13: Implement Real Logic for `WebCrawlerTool` - URL Fetching and Basic Processing

**Focus:**
This task transitions the `WebCrawlerTool` from placeholder logic to a more functional implementation for its core crawling actions (`crawl_site`, `crawl_sitemap_urls`, `crawl_markdown_file_url`). It will integrate the `crawl4ai` library (or its core concepts if direct integration is too complex for this step) for fetching web content. The sophisticated markdown generation from `crawl4ai` and the actual chunking/ingestion via `KnowledgeAgentTool` will be refined in subsequent tasks. The primary goal here is to fetch content from URLs and pass it to the (still mostly mock) `DocumentProcessor`.

**File Paths and Code Changes:**

1.  **Modify `requirements.txt`:**
    *   Add `crawl4ai` and its typical dependencies like `requests` (if not already present for other reasons).

    ```
# requirements.txt
    # ... (existing requirements including playwright)
    crawl4ai
    requests # often a dependency of web crawling or used directly
    beautifulsoup4 # For basic HTML parsing if not using full crawl4ai power yet
    # lxml # Often used by libraries like requests-html or newspaper3k
```

2.  **Modify `python/agents/web_crawler/crawler.py`:**
    *   Update `DocumentCrawler` to use `AsyncWebCrawler` from `crawl4ai` for fetching content.
    *   The methods will now perform real HTTP requests.

    ```python
# python/agents/web_crawler/crawler.py
    import asyncio
    from typing import List, Dict, Any, AsyncGenerator
    from urllib.parse import urlparse, urldefrag, urljoin # For URL manipulation
    import requests # For synchronous sitemap parsing
    from xml.etree import ElementTree # For sitemap parsing

    # Import from crawl4ai - this assumes crawl4ai is installed
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, WebCrawlerResult
        CRAWL4AI_AVAILABLE = True
    except ImportError:
        print("WebCrawlerTool: crawl4ai library not found. Using very basic fallback for URL fetching.")
        CRAWL4AI_AVAILABLE = False
        # Define a placeholder WebCrawlerResult if crawl4ai is not available
        class WebCrawlerResult:
            def __init__(self, url, success, markdown=None, html=None, links=None, error_message=None):
                self.url = url
                self.success = success
                self.markdown = markdown # In fallback, markdown might just be raw text
                self.html = html
                self.links = links or {}
                self.error_message = error_message


    class DocumentCrawler:
        """
        Manages document crawling. Uses Crawl4AI if available, otherwise basic requests.
        """
        def __init__(self, max_concurrent_crawlers: int = 5):
            self.browser_config = None
            self.run_config = None
            self.crawler_instance = None # To reuse crawler instance
            self.max_concurrent_crawlers = max_concurrent_crawlers # Not directly used by AsyncWebCrawler in this way but good for conceptual batching

            if CRAWL4AI_AVAILABLE:
                self.browser_config = BrowserConfig(
                    headless=True, 
                    verbose=False, # Keep verbose off for tool usage
                    # Add other relevant browser_config options from crawl4ai here if needed
                    # e.g., user_agent, proxy, playwright_extra_args
                     extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"], # Good for Docker
                )
                self.run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS, # Or CacheMode.USE_CACHE for repeated crawls
                    # max_pages_per_domain, max_depth can be controlled by calling methods
                    # target_elements, content_selectors if specific parsing is needed
                )
                print("DocumentCrawler: Initialized with Crawl4AI.")
            else:
                print("DocumentCrawler: Initialized with basic HTTP fetching (Crawl4AI not found).")

        async def _get_crawler(self) -> Optional[AsyncWebCrawler]:
            if not CRAWL4AI_AVAILABLE:
                return None
            if self.crawler_instance is None:
                self.crawler_instance = AsyncWebCrawler(config=self.browser_config)
                await self.crawler_instance.start() # Start the underlying browser
            return self.crawler_instance

        async def _close_crawler(self):
            if self.crawler_instance:
                await self.crawler_instance.close()
                self.crawler_instance = None

        async def _fetch_url_content(self, url: str) -> WebCrawlerResult:
            """Fetches content from a single URL using Crawl4AI or basic requests."""
            crawler = await self._get_crawler()
            if crawler:
                try:
                    result = await crawler.arun(url=url, config=self.run_config)
                    # Convert Crawl4AI's MarkdownObject to string for simplicity here
                    # Real implementation in DocumentProcessor would handle MarkdownObject.
                    markdown_content = result.markdown.raw_markdown if result.markdown else ""
                    return WebCrawlerResult(
                        url=result.url, 
                        success=result.success, 
                        markdown=markdown_content, # Store as string
                        html=result.html_content, # Crawl4AI provides html_content
                        links=result.links, 
                        error_message=result.error_message
                    )
                except Exception as e:
                    print(f"DocumentCrawler: Error fetching {url} with Crawl4AI: {e}")
                    return WebCrawlerResult(url=url, success=False, error_message=str(e))
            else: # Basic fallback
                try:
                    response = await asyncio.to_thread(requests.get, url, timeout=10)
                    response.raise_for_status()
                    # Basic link extraction (very naive)
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'html.parser')
                    links = {"internal": [], "external": []}
                    base_url_parts = urlparse(url)
                    for a_tag in soup.find_all('a', href=True):
                        href = a_tag['href']
                        joined_url = urljoin(url, href)
                        parsed_joined_url = urlparse(joined_url)
                        if parsed_joined_url.netloc == base_url_parts.netloc:
                            links["internal"].append({"href": joined_url, "text": a_tag.get_text(strip=True)})
                        else:
                            links["external"].append({"href": joined_url, "text": a_tag.get_text(strip=True)})

                    return WebCrawlerResult(url=url, success=True, markdown=response.text, html=response.text, links=links) # Using raw text as markdown
                except Exception as e:
                    print(f"DocumentCrawler: Error fetching {url} with basic requests: {e}")
                    return WebCrawlerResult(url=url, success=False, error_message=str(e))

        async def crawl_recursive(self, start_url: str, max_depth: int, max_pages: int) -> AsyncGenerator[Dict[str, Any], None]:
            """Recursively crawls a website."""
            print(f"DocumentCrawler: Starting recursive crawl for URL: {start_url}, depth: {max_depth}, pages: {max_pages}")
            
            visited_urls = set()
            queue = asyncio.Queue()
            await queue.put((start_url, 0)) # (url, current_depth)
            visited_urls.add(urldefrag(start_url)[0])
            pages_crawled_count = 0

            while not queue.empty() and pages_crawled_count < max_pages:
                current_url, current_depth = await queue.get()
                
                if current_depth > max_depth:
                    continue

                print(f"DocumentCrawler: Crawling (Rec): {current_url} at depth {current_depth}")
                result = await self._fetch_url_content(current_url)
                pages_crawled_count += 1

                if result.success:
                    yield {
                        "url": result.url,
                        "raw_content": result.markdown, # Using markdown field which might be raw HTML in fallback
                        "title": result.html[:100].split('<title>')[1].split('</title>')[0] if result.html and '<title>' in result.html else result.url.split('/')[-1] or result.url, # Basic title extraction
                        "depth": current_depth,
                        "links": result.links 
                    }
                    
                    if current_depth < max_depth:
                        for link_info in result.links.get("internal", []):
                            next_url_norm = urldefrag(link_info["href"])[0]
                            if next_url_norm not in visited_urls:
                                visited_urls.add(next_url_norm)
                                await queue.put((link_info["href"], current_depth + 1))
                else:
                    print(f"DocumentCrawler: Failed to fetch (Rec): {current_url} - {result.error_message}")
                
                if pages_crawled_count >= max_pages:
                    print(f"DocumentCrawler: Max pages ({max_pages}) reached for recursive crawl.")
                    break
            await self._close_crawler() # Close browser after crawl finishes

        async def crawl_sitemap_urls(self, urls: List[str]) -> AsyncGenerator[Dict[str, Any], None]:
            """Crawls a list of URLs, typically from a sitemap."""
            print(f"DocumentCrawler: Starting sitemap URL crawl for {len(urls)} URLs.")
            # Crawl4AI arun_many can handle this efficiently if crawler is global
            # For now, iterate and fetch one by one for simplicity with _fetch_url_content
            for i, url in enumerate(urls):
                print(f"DocumentCrawler: Crawling (Sitemap URL {i+1}/{len(urls)}): {url}")
                result = await self._fetch_url_content(url)
                if result.success:
                    yield {
                        "url": result.url,
                        "raw_content": result.markdown,
                        "title": result.html[:100].split('<title>')[1].split('</title>')[0] if result.html and '<title>' in result.html else result.url.split('/')[-1] or result.url,
                    }
                else:
                    print(f"DocumentCrawler: Failed to fetch (Sitemap URL): {url} - {result.error_message}")
            await self._close_crawler()

        async def crawl_markdown_file_url(self, url: str) -> AsyncGenerator[Dict[str, Any], None]:
            """Crawls a single markdown/text file URL."""
            print(f"DocumentCrawler: Starting markdown file crawl for URL: {url}")
            result = await self._fetch_url_content(url)
            if result.success:
                yield {
                    "url": result.url,
                    "raw_content": result.markdown, # Should be the direct content for .txt/.md
                    "title": url.split('/')[-1],
                }
            else:
                 print(f"DocumentCrawler: Failed to fetch (Markdown/TXT URL): {url} - {result.error_message}")
            await self._close_crawler()
```

3.  **Modify `python/agents/web_crawler/processors.py`:**
    *   The `DocumentProcessor.process_document` will now receive `raw_content` which might be HTML (from regular crawl) or Markdown (from .txt/.md crawl). For this task, it will still act as a simple passthrough or basic title extraction, as full markdown generation from HTML is complex.

    ```python
# python/agents/web_crawler/processors.py
    from typing import Dict, Any

    class DocumentProcessor:
        def __init__(self):
            print("DocumentProcessor initialized.")

        async def process_document(self, raw_doc_data: Dict[str, Any]) -> Dict[str, Any]:
            """
            Processes raw document data. If raw_content is HTML, it should ideally be
            converted to Markdown. For now, it passes through or does basic extraction.
            """
            url = raw_doc_data.get("url", "unknown_url")
            title = raw_doc_data.get("title", "Untitled")
            raw_content = raw_doc_data.get("raw_content", "")
            
            print(f"DocumentProcessor: Processing document from {url}")

            # Placeholder for Crawl4AI's markdown generation from HTML
            # If raw_content is HTML and we want clean markdown, Crawl4AI's
            # DefaultMarkdownGenerator or a custom one would be used.
            # For this task, we assume raw_content is either already markdown/text
            # or we use it as is (which might be HTML).
            markdown_content = raw_content 

            # If raw_content looks like HTML, we might try a very basic title extraction if not provided
            if "<html" in raw_content.lower() and title == "Untitled":
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(raw_content, 'html.parser')
                    if soup.title and soup.title.string:
                        title = soup.title.string.strip()
                except Exception as e:
                    print(f"DocumentProcessor: Basic title extraction failed for {url}: {e}")
            
            return {
                "url": url,
                "title": title,
                "markdown": markdown_content, # This is what HierarchicalChunker will use
                "metadata": {"original_url": url, "crawl_depth": raw_doc_data.get("depth", 0)}
            }
```

4.  **Verify `python/tools/web_crawler_tool.py`:**
    *   Ensure it correctly calls the updated `DocumentCrawler` methods.
    *   The ingestion part remains a placeholder (logging chunks).

    ```python
# python/tools/web_crawler_tool.py
    # ... (imports and __init__ mostly same as Task 6)
    
    # ... (_emit_crawl_event method same)

    # The execute method's calls to self.crawler methods will now use the Crawl4AI-backed versions.
    # The _process_and_ingest_crawled_doc method's call to self.processor.process_document
    # will also use the updated processor.
    # No major structural changes needed here if Task 6 was done correctly, 
    # as it already calls the async generator methods of DocumentCrawler.
```

**Dependencies/Prerequisites:**
*   Tasks 1-11 completed.
*   `crawl4ai`, `requests`, `beautifulsoup4` (for fallback) added to `requirements.txt` and installed.
*   Playwright installed with browser binaries (for `crawl4ai`'s browser-based fetching).

**Integration with Agent Zero:**
*   `DocumentCrawler` now uses `crawl4ai.AsyncWebCrawler` for robust, browser-backed fetching of web content.
*   `DocumentProcessor` receives more realistic `raw_content` (HTML or Markdown).
*   The `WebCrawlerTool`'s actions will perform actual web requests.

**Chatterbox TTS Integration Requirements for this Task:**
*   None directly.

**Docker Compatibility:**
*   Add `crawl4ai`, `requests`, `beautifulsoup4` to `requirements.txt`.
*   Ensure Playwright setup (from Task 12) is robust in Docker, as `crawl4ai` depends on it. This includes `playwright install --with-deps chromium` and any necessary system libraries for headless browser operation. The `docker/run/fs/ins/install_playwright.sh` script should cover this.

**Summary of Task 13:**
This task integrates `crawl4ai` (or a basic fetching fallback) into the `DocumentCrawler` component, enabling the `WebCrawlerTool` to fetch actual web content. The `DocumentProcessor` is updated to reflect that it now receives potentially richer content. While full markdown conversion and ingestion are still placeholders, the tool is now capable of retrieving data from the web, which is a significant step towards a functional RAG pipeline.

Please confirm to proceed.Okay, let's proceed with Task 13.

## Task 13: Implement Real Logic for `WebCrawlerTool` - URL Fetching and Basic Processing

**Focus:**
This task transitions the `WebCrawlerTool` from placeholder logic to a more functional implementation for its core crawling actions (`crawl_site`, `crawl_sitemap_urls`, `crawl_markdown_file_url`). It will integrate the `crawl4ai` library (or its core concepts if direct integration is too complex for this step) for fetching web content. The sophisticated markdown generation from `crawl4ai` and the actual chunking/ingestion via `KnowledgeAgentTool` will be refined in subsequent tasks. The primary goal here is to fetch content from URLs and pass it to the (still mostly mock) `DocumentProcessor`.

**File Paths and Code Changes:**

1.  **Modify `requirements.txt`:**
    *   Add `crawl4ai` and its typical dependencies like `requests` (if not already present for other reasons).

    ```
    # requirements.txt
    # ... (existing requirements including playwright)
    crawl4ai
    requests # often a dependency of web crawling or used directly
    beautifulsoup4 # For basic HTML parsing if not using full crawl4ai power yet
    # lxml # Often used by libraries like requests-html or newspaper3k
    ```

2.  **Modify `python/agents/web_crawler/crawler.py`:**
    *   Update `DocumentCrawler` to use `AsyncWebCrawler` from `crawl4ai` for fetching content.
    *   The methods will now perform real HTTP requests.

    ```python
    # python/agents/web_crawler/crawler.py
    import asyncio
    from typing import List, Dict, Any, AsyncGenerator
    from urllib.parse import urlparse, urldefrag, urljoin # For URL manipulation
    import requests # For synchronous sitemap parsing
    from xml.etree import ElementTree # For sitemap parsing

    # Import from crawl4ai - this assumes crawl4ai is installed
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, WebCrawlerResult
        CRAWL4AI_AVAILABLE = True
    except ImportError:
        print("WebCrawlerTool: crawl4ai library not found. Using very basic fallback for URL fetching.")
        CRAWL4AI_AVAILABLE = False
        # Define a placeholder WebCrawlerResult if crawl4ai is not available
        class WebCrawlerResult:
            def __init__(self, url, success, markdown=None, html=None, links=None, error_message=None):
                self.url = url
                self.success = success
                self.markdown = markdown # In fallback, markdown might just be raw text
                self.html = html
                self.links = links or {}
                self.error_message = error_message


    class DocumentCrawler:
        """
        Manages document crawling. Uses Crawl4AI if available, otherwise basic requests.
        """
        def __init__(self, max_concurrent_crawlers: int = 5):
            self.browser_config = None
            self.run_config = None
            self.crawler_instance = None # To reuse crawler instance
            self.max_concurrent_crawlers = max_concurrent_crawlers # Not directly used by AsyncWebCrawler in this way but good for conceptual batching

            if CRAWL4AI_AVAILABLE:
                self.browser_config = BrowserConfig(
                    headless=True, 
                    verbose=False, # Keep verbose off for tool usage
                    # Add other relevant browser_config options from crawl4ai here if needed
                    # e.g., user_agent, proxy, playwright_extra_args
                     extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"], # Good for Docker
                )
                self.run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS, # Or CacheMode.USE_CACHE for repeated crawls
                    # max_pages_per_domain, max_depth can be controlled by calling methods
                    # target_elements, content_selectors if specific parsing is needed
                )
                print("DocumentCrawler: Initialized with Crawl4AI.")
            else:
                print("DocumentCrawler: Initialized with basic HTTP fetching (Crawl4AI not found).")

        async def _get_crawler(self) -> Optional[AsyncWebCrawler]:
            if not CRAWL4AI_AVAILABLE:
                return None
            if self.crawler_instance is None:
                self.crawler_instance = AsyncWebCrawler(config=self.browser_config)
                await self.crawler_instance.start() # Start the underlying browser
            return self.crawler_instance

        async def _close_crawler(self):
            if self.crawler_instance:
                await self.crawler_instance.close()
                self.crawler_instance = None

        async def _fetch_url_content(self, url: str) -> WebCrawlerResult:
            """Fetches content from a single URL using Crawl4AI or basic requests."""
            crawler = await self._get_crawler()
            if crawler:
                try:
                    result = await crawler.arun(url=url, config=self.run_config)
                    # Convert Crawl4AI's MarkdownObject to string for simplicity here
                    # Real implementation in DocumentProcessor would handle MarkdownObject.
                    markdown_content = result.markdown.raw_markdown if result.markdown else ""
                    return WebCrawlerResult(
                        url=result.url, 
                        success=result.success, 
                        markdown=markdown_content, # Store as string
                        html=result.html_content, # Crawl4AI provides html_content
                        links=result.links, 
                        error_message=result.error_message
                    )
                except Exception as e:
                    print(f"DocumentCrawler: Error fetching {url} with Crawl4AI: {e}")
                    return WebCrawlerResult(url=url, success=False, error_message=str(e))
            else: # Basic fallback
                try:
                    response = await asyncio.to_thread(requests.get, url, timeout=10)
                    response.raise_for_status()
                    # Basic link extraction (very naive)
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'html.parser')
                    links = {"internal": [], "external": []}
                    base_url_parts = urlparse(url)
                    for a_tag in soup.find_all('a', href=True):
                        href = a_tag['href']
                        joined_url = urljoin(url, href)
                        parsed_joined_url = urlparse(joined_url)
                        if parsed_joined_url.netloc == base_url_parts.netloc:
                            links["internal"].append({"href": joined_url, "text": a_tag.get_text(strip=True)})
                        else:
                            links["external"].append({"href": joined_url, "text": a_tag.get_text(strip=True)})

                    return WebCrawlerResult(url=url, success=True, markdown=response.text, html=response.text, links=links) # Using raw text as markdown
                except Exception as e:
                    print(f"DocumentCrawler: Error fetching {url} with basic requests: {e}")
                    return WebCrawlerResult(url=url, success=False, error_message=str(e))

        async def crawl_recursive(self, start_url: str, max_depth: int, max_pages: int) -> AsyncGenerator[Dict[str, Any], None]:
            """Recursively crawls a website."""
            print(f"DocumentCrawler: Starting recursive crawl for URL: {start_url}, depth: {max_depth}, pages: {max_pages}")
            
            visited_urls = set()
            queue = asyncio.Queue()
            await queue.put((start_url, 0)) # (url, current_depth)
            visited_urls.add(urldefrag(start_url)[0])
            pages_crawled_count = 0

            while not queue.empty() and pages_crawled_count < max_pages:
                current_url, current_depth = await queue.get()
                
                if current_depth > max_depth:
                    continue

                print(f"DocumentCrawler: Crawling (Rec): {current_url} at depth {current_depth}")
                result = await self._fetch_url_content(current_url)
                pages_crawled_count += 1

                if result.success:
                    yield {
                        "url": result.url,
                        "raw_content": result.markdown, # Using markdown field which might be raw HTML in fallback
                        "title": result.html[:100].split('<title>')[1].split('</title>')[0] if result.html and '<title>' in result.html else result.url.split('/')[-1] or result.url, # Basic title extraction
                        "depth": current_depth,
                        "links": result.links 
                    }
                    
                    if current_depth < max_depth:
                        for link_info in result.links.get("internal", []):
                            next_url_norm = urldefrag(link_info["href"])[0]
                            if next_url_norm not in visited_urls:
                                visited_urls.add(next_url_norm)
                                await queue.put((link_info["href"], current_depth + 1))
                else:
                    print(f"DocumentCrawler: Failed to fetch (Rec): {current_url} - {result.error_message}")
                
                if pages_crawled_count >= max_pages:
                    print(f"DocumentCrawler: Max pages ({max_pages}) reached for recursive crawl.")
                    break
            await self._close_crawler() # Close browser after crawl finishes

        async def crawl_sitemap_urls(self, urls: List[str]) -> AsyncGenerator[Dict[str, Any], None]:
            """Crawls a list of URLs, typically from a sitemap."""
            print(f"DocumentCrawler: Starting sitemap URL crawl for {len(urls)} URLs.")
            # Crawl4AI arun_many can handle this efficiently if crawler is global
            # For now, iterate and fetch one by one for simplicity with _fetch_url_content
            for i, url in enumerate(urls):
                print(f"DocumentCrawler: Crawling (Sitemap URL {i+1}/{len(urls)}): {url}")
                result = await self._fetch_url_content(url)
                if result.success:
                    yield {
                        "url": result.url,
                        "raw_content": result.markdown,
                        "title": result.html[:100].split('<title>')[1].split('</title>')[0] if result.html and '<title>' in result.html else result.url.split('/')[-1] or result.url,
                    }
                else:
                    print(f"DocumentCrawler: Failed to fetch (Sitemap URL): {url} - {result.error_message}")
            await self._close_crawler()

        async def crawl_markdown_file_url(self, url: str) -> AsyncGenerator[Dict[str, Any], None]:
            """Crawls a single markdown/text file URL."""
            print(f"DocumentCrawler: Starting markdown file crawl for URL: {url}")
            result = await self._fetch_url_content(url)
            if result.success:
                yield {
                    "url": result.url,
                    "raw_content": result.markdown, # Should be the direct content for .txt/.md
                    "title": url.split('/')[-1],
                }
            else:
                 print(f"DocumentCrawler: Failed to fetch (Markdown/TXT URL): {url} - {result.error_message}")
            await self._close_crawler()
    ```

3.  **Modify `python/agents/web_crawler/processors.py`:**
    *   The `DocumentProcessor.process_document` will now receive `raw_content` which might be HTML (from regular crawl) or Markdown (from .txt/.md crawl). For this task, it will still act as a simple passthrough or basic title extraction, as full markdown generation from HTML is complex.

    ```python
    # python/agents/web_crawler/processors.py
    from typing import Dict, Any

    class DocumentProcessor:
        def __init__(self):
            print("DocumentProcessor initialized.")

        async def process_document(self, raw_doc_data: Dict[str, Any]) -> Dict[str, Any]:
            """
            Processes raw document data. If raw_content is HTML, it should ideally be
            converted to Markdown. For now, it passes through or does basic extraction.
            """
            url = raw_doc_data.get("url", "unknown_url")
            title = raw_doc_data.get("title", "Untitled")
            raw_content = raw_doc_data.get("raw_content", "")
            
            print(f"DocumentProcessor: Processing document from {url}")

            # Placeholder for Crawl4AI's markdown generation from HTML
            # If raw_content is HTML and we want clean markdown, Crawl4AI's
            # DefaultMarkdownGenerator or a custom one would be used.
            # For this task, we assume raw_content is either already markdown/text
            # or we use it as is (which might be HTML).
            markdown_content = raw_content 

            # If raw_content looks like HTML, we might try a very basic title extraction if not provided
            if "<html" in raw_content.lower() and title == "Untitled":
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(raw_content, 'html.parser')
                    if soup.title and soup.title.string:
                        title = soup.title.string.strip()
                except Exception as e:
                    print(f"DocumentProcessor: Basic title extraction failed for {url}: {e}")
            
            return {
                "url": url,
                "title": title,
                "markdown": markdown_content, # This is what HierarchicalChunker will use
                "metadata": {"original_url": url, "crawl_depth": raw_doc_data.get("depth", 0)}
            }
    ```

4.  **Verify `python/tools/web_crawler_tool.py`:**
    *   Ensure it correctly calls the updated `DocumentCrawler` methods.
    *   The ingestion part remains a placeholder (logging chunks).

    ```python
    # python/tools/web_crawler_tool.py
    # ... (imports and __init__ mostly same as Task 6)
    
    # ... (_emit_crawl_event method same)

    # The execute method's calls to self.crawler methods will now use the Crawl4AI-backed versions.
    # The _process_and_ingest_crawled_doc method's call to self.processor.process_document
    # will also use the updated processor.
    # No major structural changes needed here if Task 6 was done correctly, 
    # as it already calls the async generator methods of DocumentCrawler.
    ```

**Dependencies/Prerequisites:**
*   Tasks 1-11 completed.
*   `crawl4ai`, `requests`, `beautifulsoup4` (for fallback) added to `requirements.txt` and installed.
*   Playwright installed with browser binaries (for `crawl4ai`'s browser-based fetching).

**Integration with Agent Zero:**
*   `DocumentCrawler` now uses `crawl4ai.AsyncWebCrawler` for robust, browser-backed fetching of web content.
*   `DocumentProcessor` receives more realistic `raw_content` (HTML or Markdown).
*   The `WebCrawlerTool`'s actions will perform actual web requests.

**Chatterbox TTS Integration Requirements for this Task:**
*   None directly.

**Docker Compatibility:**
*   Add `crawl4ai`, `requests`, `beautifulsoup4` to `requirements.txt`.
*   Ensure Playwright setup (from Task 12) is robust in Docker, as `crawl4ai` depends on it. This includes `playwright install --with-deps chromium` and any necessary system libraries for headless browser operation. The `docker/run/fs/ins/install_playwright.sh` script should cover this.

**Summary of Task 13:**
This task integrates `crawl4ai` (or a basic fetching fallback) into the `DocumentCrawler` component, enabling the `WebCrawlerTool` to fetch actual web content. The `DocumentProcessor` is updated to reflect that it now receives potentially richer content. While full markdown conversion and ingestion are still placeholders, the tool is now capable of retrieving data from the web, which is a significant step towards a functional RAG pipeline.

Please confirm to proceed.