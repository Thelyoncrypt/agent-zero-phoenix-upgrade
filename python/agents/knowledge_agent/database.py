# python/agents/knowledge_agent/database.py
import os
import asyncio
import uuid
from typing import List, Dict, Any, Optional
import numpy as np # May not be needed if embeddings are lists
from supabase import create_client, Client as SupabaseClientLib, PostgrestAPIError
from dotenv import load_dotenv
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

# Load environment variables from the project root .env file
# This assumes the .env is at the root of the "enhanced_agent_zero" project
project_root = Path(__file__).resolve().parents[2]
dotenv_path = project_root / '.env'
load_dotenv(dotenv_path, override=True)

# Cosine similarity can be removed if pgvector handles it, but good for reference
def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """Computes cosine similarity between two vectors."""
    vec1 = np.array(v1)
    vec2 = np.array(v2)
    if vec1.shape != vec2.shape or vec1.ndim != 1:
        print(f"Warning: cosine_similarity received vectors of mismatched shapes/dims. v1: {vec1.shape}, v2: {vec2.shape}")
        return 0.0
    dot_product = np.dot(vec1, vec2)
    norm_v1 = np.linalg.norm(vec1)
    norm_v2 = np.linalg.norm(vec2)
    if norm_v1 == 0 or norm_v2 == 0:
        return 0.0
    return dot_product / (norm_v1 * norm_v2)

class DatabaseManager:
    """
    Manages interaction with the Supabase vector database using pgvector.
    """
    def __init__(self, supabase_url: Optional[str] = None, supabase_key: Optional[str] = None):
        self.url = supabase_url or os.getenv("SUPABASE_URL")
        self.key = supabase_key or os.getenv("SUPABASE_KEY") # This should be the service_role key for inserts/admin

        if not self.url or not self.key:
            logger.error("Supabase URL and Key must be provided via environment variables or arguments.")
            raise ValueError("Supabase URL and Key must be provided.")

        try:
            self.client: SupabaseClientLib = create_client(self.url, self.key)
            logger.info(f"DatabaseManager: Successfully connected to Supabase instance at {self.url[:30]}...")
            # Test connection (optional, e.g., by trying to fetch a small, known piece of data or schema)
        except Exception as e:
            logger.error(f"DatabaseManager: Failed to initialize Supabase client: {e}", exc_info=True)
            raise

    async def store_chunks(self, chunks_data: List[Dict[str, Any]], batch_size: int = 50) -> List[Any]: # Return list of Supabase IDs
        """
        Stores chunks with their embeddings and metadata in Supabase.
        chunks_data: list of dicts, each like
                     {"text": str, "embedding": List[float], "metadata": Dict, "id": str (client-side id, not db id)}
        The actual DB `id` will be auto-generated by Supabase.
        The `url` and `chunk_number` for the DB table should be in `metadata`.
        """
        if not chunks_data:
            return []

        records_to_insert = []
        for chunk_info in chunks_data:
            metadata = chunk_info.get("metadata", {})
            source_url = metadata.get("source_url", metadata.get("url", f"unknown_source_{str(uuid.uuid4())}"))
            chunk_idx = metadata.get("chunk_index", 0) # Ensure chunk_index is present

            if not chunk_info.get("text") or not chunk_info.get("embedding"):
                logger.warning(f"DatabaseManager: Skipping chunk for {source_url} due to missing text or embedding.")
                continue

            records_to_insert.append({
                "url": source_url,
                "chunk_number": chunk_idx,
                "content": chunk_info["text"],
                "embedding": chunk_info["embedding"], # pgvector handles list of floats
                "metadata": metadata # Store all other metadata
            })

        if not records_to_insert:
            logger.info("DatabaseManager: No valid records to insert after pre-processing.")
            return []

        all_inserted_ids = []
        logger.info(f"DatabaseManager: Attempting to store {len(records_to_insert)} chunks in Supabase in batches of {batch_size}.")

        for i in range(0, len(records_to_insert), batch_size):
            batch = records_to_insert[i:i+batch_size]
            try:
                # The Supabase Python client's insert is synchronous. Use to_thread.
                response = await asyncio.to_thread(
                    self.client.table("rag_pages").insert(batch).execute
                )
                if response.data:
                    inserted_ids_batch = [record['id'] for record in response.data]
                    all_inserted_ids.extend(inserted_ids_batch)
                    logger.info(f"DatabaseManager: Stored batch {i//batch_size + 1}, {len(inserted_ids_batch)} chunks in Supabase.")
                else:
                    error_info = getattr(response, 'error', "Unknown error during insert")
                    logger.error(f"DatabaseManager: Failed to store batch {i//batch_size + 1}. Supabase error: {error_info}. Data (first item): {batch[0] if batch else 'N/A'}")
            except PostgrestAPIError as pae: # Specific Supabase error
                logger.error(f"DatabaseManager: PostgrestAPIError storing batch {i//batch_size + 1}: {pae.message} (Code: {pae.code}, Details: {pae.details}, Hint: {pae.hint})", exc_info=True)
            except Exception as e:
                logger.error(f"DatabaseManager: General exception storing batch {i//batch_size + 1}: {e}", exc_info=True)

        logger.info(f"DatabaseManager: Finished storing chunks. Total successfully inserted IDs: {len(all_inserted_ids)}.")
        return all_inserted_ids

    async def semantic_search(
        self,
        query_embedding: List[float],
        limit: int,
        filter_metadata: Optional[Dict] = None # This is the `filter` arg for the RPC function
    ) -> List[Dict[str, Any]]:
        """Performs semantic search using the match_rag_pages RPC function."""
        if not query_embedding or sum(abs(x) for x in query_embedding) == 0:
            logger.warning("DatabaseManager: Query embedding is zero or empty. Returning no results.")
            return []

        rpc_params = {
            "query_embedding": query_embedding, # Must match SQL function param name
            "match_count": limit,             # Must match SQL function param name
            "filter": filter_metadata or {}   # Must match SQL function param name, ensure it's a dict
        }

        logger.debug(f"DatabaseManager: Calling RPC 'match_rag_pages' with params: query_embedding_dim={len(query_embedding)}, match_count={limit}, filter={rpc_params['filter']}")

        try:
            response = await asyncio.to_thread(
                self.client.rpc("match_rag_pages", rpc_params).execute
            )
            if response.data:
                logger.info(f"DatabaseManager: RPC 'match_rag_pages' returned {len(response.data)} results.")
                # The RPC function should return: id, url, chunk_number, content, metadata, similarity
                return response.data
            else:
                error_info = getattr(response, 'error', "Unknown error from RPC")
                logger.warning(f"DatabaseManager: No results or error from RPC 'match_rag_pages'. Error: {error_info}")
                return []
        except PostgrestAPIError as pae:
             logger.error(f"DatabaseManager: PostgrestAPIError during RPC 'match_rag_pages': {pae.message}", exc_info=True)
        except Exception as e:
            logger.error(f"DatabaseManager: Exception during RPC 'match_rag_pages': {e}", exc_info=True)
        return []

    async def get_all_sources(self) -> List[str]:
        """Fetches all unique 'url' values from the rag_pages table (which represent document sources)."""
        try:
            # This might be slow for very large tables. Consider if a dedicated sources table is needed.
            # For now, a distinct select on url.
            # The Python client might not directly support DISTINCT in select string easily for ORM-like methods.
            # Using RPC or a view might be better for large scale.
            # A simpler approach for smaller datasets:
            response = await asyncio.to_thread(
                self.client.table("rag_pages").select("url", count="exact").execute
            )
            # The above gets all URLs, then we make them unique in Python.
            # If performance is an issue, create a view or function in Supabase for distinct URLs.

            if response.data:
                sources = sorted(list(set(item["url"] for item in response.data if item and "url" in item)))
                logger.info(f"DatabaseManager: Retrieved {len(sources)} unique sources.")
                return sources
            else:
                error_info = getattr(response, 'error', "Unknown error")
                logger.warning(f"DatabaseManager: Failed to get sources. Error: {error_info}")
                return []
        except Exception as e:
            logger.error(f"DatabaseManager: Exception during get_all_sources: {e}", exc_info=True)
            return []